{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9skw-_7MSGV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import regex\n",
        "import shutil\n",
        "from typing import Any, Tuple, Optional\n",
        "from google.colab import drive\n",
        "\n",
        "# drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbxOQrqZT6rg"
      },
      "source": [
        "## Helper Functions for Word Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0yrMod9MsEr"
      },
      "outputs": [],
      "source": [
        "def count_word_multilingual(text: str) -> int:\n",
        "    \"\"\"\n",
        "    Performs rough tokenization of the input text and counts the number of tokens.\n",
        "    Supports mixed languages including English, Chinese (including Japanese Kanji),\n",
        "    Spanish, Portuguese, French, German, Russian, Norwegian, and Japanese.\n",
        "\n",
        "    Rules:\n",
        "    1) [0-9\\\\.]+ : Matches consecutive digits (including decimal points) as a single token.\n",
        "    2) \\\\p{Han} : Matches a single Chinese character (including Kanji used in Japanese).\n",
        "    3) \\\\p{Hiragana}+ : Matches consecutive Hiragana characters as a single token.\n",
        "    4) \\\\p{Katakana}+ : Matches consecutive Katakana characters as a single token.\n",
        "    5) \\\\p{Cyrillic}+ : Matches consecutive Cyrillic letters (Russian).\n",
        "    6) \\\\p{Latin}+ : Matches consecutive Latin letters (including diacritics),\n",
        "                     supporting English, Spanish, Portuguese, French, German, Norwegian, etc.\n",
        "\n",
        "    Notes:\n",
        "    - Each Chinese character (\\\\p{Han}) is treated as an individual token.\n",
        "      For example, \"你好\" => [\"你\", \"好\"].\n",
        "    - Consecutive characters from other scripts (e.g., \"hello\") are treated as a single token.\n",
        "    - This is a simplified example and does not handle other symbols, punctuation,\n",
        "      or complex numerical formats.\n",
        "    - Requires the third-party module `regex` (pip install regex),\n",
        "      because the built-in `re` module has incomplete support for Unicode properties \\\\p{...}.\n",
        "    \"\"\"\n",
        "    pattern = (\n",
        "        r'[0-9\\.]+'        # Consecutive digits and decimal points\n",
        "        r'|\\p{Han}'        # Single Chinese character\n",
        "        r'|\\p{Hiragana}+'  # Consecutive Hiragana characters\n",
        "        r'|\\p{Katakana}+'  # Consecutive Katakana characters\n",
        "        r'|\\p{Cyrillic}+'  # Consecutive Cyrillic letters\n",
        "        r'|\\p{Latin}+'     # Consecutive Latin letters (including diacritics)\n",
        "    )\n",
        "    tokens = regex.findall(pattern, text)\n",
        "    return tokens, len(tokens)\n",
        "\n",
        "def sort_files(files_list):\n",
        "\n",
        "    num_re = re.compile(r\"^(\\d+)\")  # capture 1+ digits at start\n",
        "\n",
        "    def leading_num(fname):\n",
        "        m = num_re.match(fname)\n",
        "        if m:\n",
        "            return int(m.group(1))\n",
        "        else:\n",
        "            return float(\"inf\")  # or 0, depending on where you want no‑number files\n",
        "\n",
        "    sorted_files = sorted(files_list, key=leading_num)\n",
        "\n",
        "    return sorted_files\n",
        "\n",
        "def split_by_analysis(\n",
        "    instructions: str,\n",
        "    prediction: str,\n",
        "    record_id: Any\n",
        ") -> Tuple[str, Optional[str], Any]:\n",
        "    \"\"\"\n",
        "    Split a model's output into (analysis, result) based on markers.\n",
        "\n",
        "    Specifically designed for the format:\n",
        "    Analysis:\n",
        "    [CoT reasoning]\n",
        "    Result:\n",
        "    [answer]\n",
        "\n",
        "    Returns the CoT reasoning (without the \"Analysis:\" label) and the result.\n",
        "    \"\"\"\n",
        "    # 1: Split by \"</think>\" for reasoning models\n",
        "    try:\n",
        "        parts = prediction.split(\"</think>\", 1)\n",
        "        if len(parts) == 2:\n",
        "            analysis = parts[0].strip()\n",
        "\n",
        "            # Remove \"Analysis:\" prefix if present\n",
        "            analysis = re.sub(r'^Analysis:\\s*', '', analysis, flags=re.IGNORECASE).strip()\n",
        "\n",
        "            # Remove \"<think>\" prefix if present\n",
        "            analysis = re.sub(r'<think>', '', analysis, flags=re.IGNORECASE).strip()\n",
        "\n",
        "            result = parts[1].strip()\n",
        "            return analysis, result, record_id\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    # 2: Split by \"Result:\" for non-reasoning models\n",
        "    try:\n",
        "        parts = prediction.split(\"Result:\", 1)\n",
        "        if len(parts) == 2:\n",
        "            analysis = parts[0].strip()\n",
        "\n",
        "            # Remove \"Analysis:\" prefix if present\n",
        "            analysis = re.sub(r'^Analysis:\\s*', '', analysis, flags=re.IGNORECASE).strip()\n",
        "\n",
        "            result = parts[1].strip()\n",
        "            return analysis, result, record_id\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    # 3: If \"Result:\" does not exist, derive the label from instructions\n",
        "    match = re.search(r\"Result:\\s*([^:]+?):\", instructions)  # find the label in \"Result: [label]:...\"\n",
        "    if match:\n",
        "        label = match.group(1).strip()\n",
        "        marker = f\"{label}:\"  # [label]:...\n",
        "        try:\n",
        "            analysis, result = prediction.split(marker, 1)  # look for this label in the prediction\n",
        "\n",
        "            # Remove \"Analysis:\" prefix if present\n",
        "            analysis = re.sub(r'^Analysis:\\s*', '', analysis.strip(), flags=re.IGNORECASE).strip()\n",
        "            return analysis, result, record_id\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "\n",
        "    # 4: Otherwise, return the whole output (prediction) as the analysis\n",
        "    analysis = prediction.strip()\n",
        "\n",
        "    return analysis, None, record_id\n",
        "\n",
        "\n",
        "def aug_cot_length(json_path):\n",
        "    \"\"\"\n",
        "    Given a json file of data, return the number of tokens in the analysis and result\n",
        "    and add to each entry.\n",
        "\n",
        "    RETURNS:\n",
        "        - Augments the json file with the length of CoT analysis\n",
        "        - returns the same list of dictionaries\n",
        "    \"\"\"\n",
        "\n",
        "    with open(json_path, 'r') as f:\n",
        "        json_content = json.load(f)\n",
        "\n",
        "    for i, d in enumerate(json_content):\n",
        "        try:\n",
        "            analysis, result, _ = split_by_analysis(d[\"instruction\"], d[\"pred\"], d[\"id\"])\n",
        "\n",
        "            if analysis:\n",
        "                _, num_tokens = count_word_multilingual(analysis)\n",
        "                d['CoT Length'] = num_tokens\n",
        "            else:\n",
        "                # No analysis found, set to 0 or handle as needed\n",
        "                d['CoT Length'] = 0\n",
        "                print(f\"Warning: No analysis found for record {i} (id: {d.get('id', 'unknown')})\")\n",
        "\n",
        "        except KeyError as e:\n",
        "            print(f\"Error: Missing key {e} in record {i}\")\n",
        "            d['CoT Length'] = 0\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing record {i}: {e}\")\n",
        "            d['CoT Length'] = 0\n",
        "\n",
        "    return json_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmq-bpDeTxze"
      },
      "source": [
        "## Migrate the \"Direct\" Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acE6ahIeRVyj",
        "outputId": "fc60f319-f505-49dc-b255-9fd7355bac63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DONE\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "# For direct\n",
        "def copy_files_with_rsync():\n",
        "    for task_name in os.listdir(path):\n",
        "        task_path = os.path.join(path, task_name)\n",
        "\n",
        "        for model in os.listdir(task_path):\n",
        "            model_path = os.path.join(task_path, model)\n",
        "\n",
        "            direct_file = f\"{task_name}-direct-greedy-42.result.json\"\n",
        "            direct_src_file = os.path.join(model_path, direct_file)\n",
        "            dst_file = os.path.join(os.path.join(destination_path, task_name, model), f\"{direct_file}\")\n",
        "\n",
        "            if os.path.exists(direct_src_file):\n",
        "                try:\n",
        "                    # Ensure destination directory exists\n",
        "                    os.makedirs(os.path.dirname(dst_file), exist_ok=True)\n",
        "\n",
        "                    # Use rsync with proper path handling for spaces\n",
        "                    subprocess.run([\n",
        "                        'rsync', '-av', '--mkpath',  # --mkpath creates destination directories\n",
        "                        direct_src_file, dst_file\n",
        "                    ], check=True)\n",
        "                    # print(f\"Synced: {dst_file}\")\n",
        "                except subprocess.CalledProcessError as e:\n",
        "                    print(f\"Error syncing {direct_src_file}: {e}\")\n",
        "                    # Fallback to cp if rsync fails\n",
        "                    try:\n",
        "                        subprocess.run(['cp', direct_src_file, dst_file], check=True)\n",
        "                        print(f\"Fallback copied: {dst_file}\")\n",
        "                    except subprocess.CalledProcessError as e2:\n",
        "                        print(f\"Fallback also failed: {e2}\")\n",
        "            else:\n",
        "                print(f\"File not found: {direct_src_file}\")\n",
        "\n",
        "print(\"DONE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TebeAx3jT0sY"
      },
      "source": [
        "## Migrate the CoT files and augment with CoT output length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYuenwQQkigy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "# For direct\n",
        "\n",
        "path = '/content/drive/MyDrive/cot-analysis/combined-results'\n",
        "\n",
        "destination_path = '/content/drive/MyDrive/cot-analysis/cot-length'\n",
        "\n",
        "def copy_cot_files_with_rsync():\n",
        "    for task_name in os.listdir(path):\n",
        "        task_path = os.path.join(path, task_name)\n",
        "\n",
        "        for model in os.listdir(task_path):\n",
        "            model_path = os.path.join(task_path, model)\n",
        "\n",
        "            cot_file = f\"{task_name}-cot-greedy-42.result.json\"\n",
        "            cot_src_file = os.path.join(model_path, cot_file)  # Path to source file\n",
        "\n",
        "            dst_file = os.path.join(os.path.join(destination_path, task_name, model), f\"{cot_file}\")  # Destination\n",
        "\n",
        "            if os.path.exists(cot_src_file):\n",
        "                try:\n",
        "                    # Process the file and get augmented JSON data\n",
        "                    augmented_cot_data = aug_cot_length(cot_src_file)  # json content\n",
        "\n",
        "                    # Ensure destination directory exists\n",
        "                    os.makedirs(os.path.dirname(dst_file), exist_ok=True)\n",
        "\n",
        "                    # Save the augmented JSON data to destination file\n",
        "                    with open(dst_file, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(augmented_cot_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {cot_src_file}: {e}\")\n",
        "            else:\n",
        "                print(f\"File not found: {cot_src_file}\")\n",
        "\n",
        "# Run the processing\n",
        "print(\"Starting CoT length analysis and file creation...\")\n",
        "copy_cot_files_with_rsync()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
