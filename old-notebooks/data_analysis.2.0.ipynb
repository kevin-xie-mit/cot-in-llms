{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91570529",
   "metadata": {},
   "source": [
    "# Figure Creation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b03da74",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a77b76",
   "metadata": {},
   "source": [
    "## 0. Helpful Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b82b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def load_sheet(sheet_path):\n",
    "    '''\n",
    "    Input: path to excel sheet\n",
    "    Output: pandas dataframe of entire excel spreadsheet\n",
    "    '''\n",
    "    sheet = pd.ExcelFile(sheet_path)\n",
    "\n",
    "    return sheet\n",
    "\n",
    "\n",
    "sheet_path = \"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet.xlsx\"\n",
    "google_sheet = load_sheet(sheet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed214877",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size_mapping = {'DeepSeek-R1-Distill-Qwen-1.5B': 1.5, 'DeepSeek-R1-Distill-Qwen-7B': 7,\n",
    "                       'DeepSeek-R1-Distill-Llama-8B': 8, 'DeepSeek-R1-Distill-Qwen-14B': 14,\n",
    "                         'DeepSeek-R1-Distill-Qwen-32B': 32, 'DeepSeek-R1-Distill-Llama-70B': 70, \n",
    "                         'DeepSeek-R1': 671, 'Baichuan-M1-14B-Instruct': 14, 'gemma-2-9b-it': 9, \n",
    "                         'gemma-2-27b-it': 27, 'gemma-3-1b-it': 1, 'gemma-3-4b-it': 4, 'gemma-3-12b-it': 12, \n",
    "                         'gemma-3-27b-it': 27, 'Llama-3.1-8B-Instruct': 8, 'Llama-3.1-70B-Instruct': 70, 'Llama-3.2-1B-Instruct': 1, 'Llama-3.2-3B-Instruct': 3, 'Llama-3.3-70B-Instruct': 70, 'Llama-4-Scout-17B-16E-Instruct': 109, 'Llama-3.1-Nemotron-70B-Instruct-HF': 70, 'meditron-7b': 7, 'meditron-70b': 70, 'MeLLaMA-13B-chat': 13, 'MeLLaMA-70B-chat': 70, 'Llama3-OpenBioLLM-8B': 8, 'Llama3-OpenBioLLM-70B': 70, 'MMed-Llama-3-8B': 8, 'Llama-3.1-8B-UltraMedical': 8, 'Llama-3-70B-UltraMedical': 70, 'Ministral-8B-Instruct-2410': 8, 'Mistral-Small-Instruct-2409': 22, 'Mistral-Small-24B-Instruct-2501': 24, 'Mistral-Small-3.1-24B-Instruct-2503': 24, 'Mistral-Large-Instruct-2411': 123, 'BioMistral-7B': 7, 'Phi-3.5-mini-instruct': 4, 'Phi-3.5-MoE-instruct': 42, 'Phi-4': 14, 'Qwen2.5-1.5B-Instruct': 1.5, 'Qwen2.5-3B-Instruct': 3, 'Qwen2.5-7B-Instruct': 7, 'Qwen2.5-72B-Instruct': 72, 'QwQ-32B-Preview': 32, 'QWQ-32B': 32, 'Athene-V2-Chat': 72, 'Yi-1.5-9B-Chat-16K': 9, 'Yi-1.5-34B-Chat-16K': 34, 'gpt-35-turbo-0125': '/', 'gpt-4o-0806': '/', 'gemini-2.0-flash-001': '/', 'gemini-1.5-pro-002': '/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4eb532",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"DeepSeek-R1\",\n",
    "    \"gpt-4o-0806\",\n",
    "    \"gemini-1.5-pro-002\",\n",
    "    \"gemini-2.0-flash-001\",\n",
    "    \"Athene-V2-Chat\",\n",
    "    \"Mistral-Large-Instruct-2411\",\n",
    "    \"Qwen2.5-72B-Instruct\",\n",
    "    \"gemma-3-27b-it\",\n",
    "    \"Llama-3.3-70B-Instruct\",\n",
    "    \"DeepSeek-R1-Distill-Llama-70B\",\n",
    "    \"Llama-3.1-70B-Instruct\",\n",
    "    \"QWQ-32B\",\n",
    "    \"DeepSeek-R1-Distill-Qwen-32B\",\n",
    "    \"gemma-3-12b-it\",\n",
    "    \"Baichuan-M1-14B-Instruct\",\n",
    "    \"gemma-2-27b-it\",\n",
    "    \"Phi-4\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503\",\n",
    "    \"gpt-35-turbo-0125\",\n",
    "    \"DeepSeek-R1-Distill-Qwen-14B\",\n",
    "    \"Mistral-Small-24B-Instruct-2501\",\n",
    "    \"gemma-2-9b-it\",\n",
    "    \"Llama-3-70B-UltraMedical\",\n",
    "    \"Llama3-OpenBioLLM-70B\",\n",
    "    \"Mistral-Small-Instruct-2409\",\n",
    "    \"Llama-4-Scout-17B-16E-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Yi-1.5-34B-Chat-16K\",\n",
    "    \"QwQ-32B-Preview\",\n",
    "    \"Llama-3.1-8B-Instruct\",\n",
    "    \"Llama-3.1-Nemotron-70B-Instruct-HF\",\n",
    "    \"Ministral-8B-Instruct-2410\",\n",
    "    \"MeLLaMA-70B-chat\",\n",
    "    \"gemma-3-4b-it\",\n",
    "    \"Yi-1.5-9B-Chat-16K\",\n",
    "    \"Phi-3.5-MoE-instruct\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"Llama-3.2-3B-Instruct\",\n",
    "    \"Phi-3.5-mini-instruct\",\n",
    "    \"MMed-Llama-3-8B\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    \"MeLLaMA-13B-chat\",\n",
    "    \"Llama-3.1-8B-UltraMedical\",\n",
    "    \"Llama3-OpenBioLLM-8B\",\n",
    "    \"meditron-70b\",\n",
    "    \"gemma-3-1b-it\",\n",
    "    \"BioMistral-7B\",\n",
    "    \"DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"Llama-3.2-1B-Instruct\",\n",
    "    \"meditron-7b\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46adfae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_english_tasks = {\n",
    "    \"ADE-Identification\",\n",
    "    \"ADE-Extraction\",\n",
    "    \"ADE-Drug dosage\",\n",
    "    \"BrainMRI-AIS\",\n",
    "    \"ClinicalNotes-UPMC\",\n",
    "    \"CLIP\",\n",
    "    \"GOUT-CC-Consensus\",\n",
    "    \"n2c2 2006-De-identification\",\n",
    "    \"Medication extraction\",\n",
    "    \"n2c2 2010-Concept\",\n",
    "    \"n2c2 2010-Assertion\",\n",
    "    \"n2c2 2010-Relation\",\n",
    "    \"n2c2 2014-De-identification\",\n",
    "    \"MEDIQA 2019-RQE\",\n",
    "    \"MedNLI\",\n",
    "    \"MedSTS\",\n",
    "    \"MTS\",\n",
    "    \"MTS-Temporal\",\n",
    "    \"n2c2 2018-ADE&medication\",\n",
    "    \"MEDIQA 2023-chat-A\",\n",
    "    \"MEDIQA 2023-sum-A\",\n",
    "    \"MEDIQA 2023-sum-B\",\n",
    "    \"n2c2 2014-Diabetes\",\n",
    "    \"n2c2 2014-CAD\",\n",
    "    \"n2c2 2014-Hyperlipidemia\",\n",
    "    \"n2c2 2014-Hypertension\",\n",
    "    \"n2c2 2014-Medication\",\n",
    "    \"icliniq-10k\",\n",
    "    \"HealthCareMagic-100k\",\n",
    "    \"MIMIC-IV CDM\",\n",
    "    \"MIMIC-III Outcome.LoS\",\n",
    "    \"MIMIC-III Outcome.Mortality\",\n",
    "    \"MIMIC-IV BHC\",\n",
    "    \"MIMIC-IV DiReCT.Dis\",\n",
    "    \"MIMIC-IV DiReCT.PDD\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dce705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Baichuan-M1-14B-Instruct': 'Medical', 'DeepSeek-R1': 'General', 'DeepSeek-R1-Distill-Llama-8B': 'General', 'DeepSeek-R1-Distill-Llama-70B': 'General', 'DeepSeek-R1-Distill-Qwen-1.5B': 'General', 'DeepSeek-R1-Distill-Qwen-7B': 'General', 'DeepSeek-R1-Distill-Qwen-14B': 'General', 'DeepSeek-R1-Distill-Qwen-32B': 'General', 'gemma-2-9b-it': 'General', 'gemma-2-27b-it': 'General', 'gemma-3-1b-it': 'General', 'gemma-3-4b-it': 'General', 'gemma-3-12b-it': 'General', 'gemma-3-27b-it': 'General', 'Llama-3.1-8B-Instruct': 'General', 'Llama-3.1-70B-Instruct': 'General', 'Llama-3.2-1B-Instruct': 'General', 'Llama-3.2-3B-Instruct': 'General', 'Llama-3.3-70B-Instruct': 'General', 'Llama-4-Scout-17B-16E-Instruct': 'General', 'Llama-3.1-Nemotron-70B-Instruct-HF': 'General', 'meditron-7b': 'Medical', 'meditron-70b': 'Medical', 'MeLLaMA-13B-chat': 'Medical', 'MeLLaMA-70B-chat': 'Medical', 'Llama3-OpenBioLLM-8B': 'Medical', 'Llama3-OpenBioLLM-70B': 'Medical', 'MMed-Llama-3-8B': 'Medical', 'Llama-3.1-8B-UltraMedical': 'Medical', 'Llama-3-70B-UltraMedical': 'Medical', 'Ministral-8B-Instruct-2410': 'General', 'Mistral-Small-Instruct-2409': 'General', 'Mistral-Small-24B-Instruct-2501': 'General', 'Mistral-Small-3.1-24B-Instruct-2503': 'General', 'Mistral-Large-Instruct-2411': 'General', 'BioMistral-7B': 'Medical', 'Phi-3.5-mini-instruct': 'General', 'Phi-3.5-MoE-instruct': 'General', 'Phi-4': 'General', 'Qwen2.5-1.5B-Instruct': 'General', 'Qwen2.5-3B-Instruct': 'General', 'Qwen2.5-7B-Instruct': 'General', 'Qwen2.5-72B-Instruct': 'General', 'QwQ-32B-Preview': 'General', 'QWQ-32B': 'General', 'Athene-V2-Chat': 'General', 'Yi-1.5-9B-Chat-16K': 'General', 'Yi-1.5-34B-Chat-16K': 'General', 'gpt-35-turbo-0125': 'General', 'gpt-4o-0806': 'General', 'gemini-2.0-flash-001': 'General', 'gemini-1.5-pro-002': 'General'}\n"
     ]
    }
   ],
   "source": [
    "data = google_sheet.parse(\"All-Sheet\")\n",
    "\n",
    "model_domain_mapping = {}\n",
    "\n",
    "for row_idx, model_name in enumerate(data[\"Model Name\"]):\n",
    "    if model_name not in model_domain_mapping:\n",
    "        domain = data[\"Model Domain\"][row_idx]\n",
    "        if domain == \"gen\":\n",
    "            domain = \"General\"\n",
    "        elif domain == \"med\":\n",
    "            domain = \"Medical\"\n",
    "        model_domain_mapping[model_name] = domain\n",
    "\n",
    "print(model_domain_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e98faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accessibility_mapping = {\n",
    "    \"Baichuan-M1-14B-Instruct\": \"open source\",\n",
    "    \"DeepSeek-R1\": \"open source\",\n",
    "    \"DeepSeek-R1-Distill-Llama-8B\": \"open source\",\n",
    "    \"DeepSeek-R1-Distill-Llama-70B\": \"open source\",\n",
    "    \"DeepSeek-R1-Distill-Qwen-1.5B\": \"open source\",\n",
    "    \"DeepSeek-R1-Distill-Qwen-7B\": \"open source\",\n",
    "    \"DeepSeek-R1-Distill-Qwen-14B\": \"open source\",\n",
    "    \"DeepSeek-R1-Distill-Qwen-32B\": \"open source\",\n",
    "    \"gemma-2-9b-it\": \"open source\",\n",
    "    \"gemma-2-27b-it\": \"open source\",\n",
    "    \"gemma-3-1b-it\": \"open source\",\n",
    "    \"gemma-3-4b-it\": \"open source\",\n",
    "    \"gemma-3-12b-it\": \"open source\",\n",
    "    \"gemma-3-27b-it\": \"open source\",\n",
    "    \"Llama-3.1-8B-Instruct\": \"open source\",\n",
    "    \"Llama-3.1-70B-Instruct\": \"open source\",\n",
    "    \"Llama-3.2-1B-Instruct\": \"open source\",\n",
    "    \"Llama-3.2-3B-Instruct\": \"open source\",\n",
    "    \"Llama-3.3-70B-Instruct\": \"open source\",\n",
    "    \"Llama-4-Scout-17B-16E-Instruct\": \"open source\",\n",
    "    \"Llama-3.1-Nemotron-70B-Instruct-HF\": \"open source\",\n",
    "    \"meditron-7b\": \"open source\",\n",
    "    \"meditron-70b\": \"open source\",\n",
    "    \"MeLLaMA-13B-chat\": \"open source\",\n",
    "    \"MeLLaMA-70B-chat\": \"open source\",\n",
    "    \"Llama3-OpenBioLLM-8B\": \"open source\",\n",
    "    \"Llama3-OpenBioLLM-70B\": \"open source\",\n",
    "    \"MMed-Llama-3-8B\": \"open source\",\n",
    "    \"Llama-3.1-8B-UltraMedical\": \"open source\",\n",
    "    \"Llama-3-70B-UltraMedical\": \"open source\",\n",
    "    \"Ministral-8B-Instruct-2410\": \"open source\",\n",
    "    \"Mistral-Small-Instruct-2409\": \"open source\",\n",
    "    \"Mistral-Small-24B-Instruct-2501\": \"open source\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503\": \"open source\",\n",
    "    \"Mistral-Large-Instruct-2411\": \"open source\",\n",
    "    \"BioMistral-7B\": \"open source\",\n",
    "    \"Phi-3.5-mini-instruct\": \"open source\",\n",
    "    \"Phi-3.5-MoE-instruct\": \"open source\",\n",
    "    \"Phi-4\": \"open source\",\n",
    "    \"Qwen2.5-1.5B-Instruct\": \"open source\",\n",
    "    \"Qwen2.5-3B-Instruct\": \"open source\",\n",
    "    \"Qwen2.5-7B-Instruct\": \"open source\",\n",
    "    \"Qwen2.5-72B-Instruct\": \"open source\",\n",
    "    \"QwQ-32B-Preview\": \"open source\",\n",
    "    \"QWQ-32B\": \"open source\",\n",
    "    \"Athene-V2-Chat\": \"open source\",\n",
    "    \"Yi-1.5-9B-Chat-16K\": \"open source\",\n",
    "    \"Yi-1.5-34B-Chat-16K\": \"open source\",\n",
    "    \"gemini-1.5-pro-002\": \"commercial\",\n",
    "    \"gemini-2.0-flash-001\": \"commercial\",\n",
    "    \"gpt-4o-0806\": \"commercial\",\n",
    "    \"gpt-35-turbo-0125\": \"commercial\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5363bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_type_map = {\n",
    "    \"ADE-Identification\": \"Text Classification\",\n",
    "    \"ADE-Extraction\": \"Event Extraction\",\n",
    "    \"ADE-Drug dosage\": \"Event Extraction\",\n",
    "    \"BARR2\": \"Event Extraction\",\n",
    "    \"BrainMRI-AIS\": \"Text Classification\",\n",
    "    \"Brateca-Hospitalization\": \"Text Classification\",\n",
    "    \"Brateca-Mortality\": \"Text Classification\",\n",
    "    \"Cantemist-Coding\": \"Normalization and Coding\",\n",
    "    \"Cantemis-NER\": \"Named Entity Recognition\",\n",
    "    \"Cantemis-Norm\": \"Normalization and Coding\",\n",
    "    \"CARES-Area\": \"Text Classification\",\n",
    "    \"CARES ICD10 Block\": \"Normalization and Coding\",\n",
    "    \"CARES-ICD10 Chapter\": \"Normalization and Coding\",\n",
    "    \"CARES-ICD10 Subblock\": \"Normalization and Coding\",\n",
    "    \"CHIP-CDEE\": \"Event Extraction\",\n",
    "    \"C-EMRS\": \"Text Classification\",\n",
    "    \"CodiEsp-ICD-10-CM\": \"Normalization and Coding\",\n",
    "    \"CodiEsp-ICD-10-PCS\": \"Normalization and Coding\",\n",
    "    \"ClinicalNotes-UPMC\": \"Text Classification\",\n",
    "    \"PPTS\": \"Text Classification\",\n",
    "    \"CLINpt-NER\": \"Named Entity Recognition\",\n",
    "    \"CLIP\": \"Text Classification\",\n",
    "    \"cMedQA\": \"Question Answering\",\n",
    "    \"DialMed\": \"Text Classification\",\n",
    "    \"DiSMed-NER\": \"Named Entity Recognition\",\n",
    "    \"MIE\": \"Event Extraction\",\n",
    "    \"EHRQA-Primary department\": \"Text Classification\",\n",
    "    \"EHRQA-QA\": \"Question Answering\",\n",
    "    \"EHRQA-Sub department\": \"Text Classification\",\n",
    "    \"Ex4CDS\": \"Named Entity Recognition\",\n",
    "    \"GOUT-CC-Consensus\": \"Text Classification\",\n",
    "    \"n2c2 2006-De-identification\": \"Named Entity Recognition\",\n",
    "    \"Medication extraction\": \"Event Extraction\",\n",
    "    \"n2c2 2010-Concept\": \"Named Entity Recognition\",\n",
    "    \"n2c2 2010-Assertion\": \"Named Entity Recognition\",\n",
    "    \"n2c2 2010-Relation\": \"Event Extraction\",\n",
    "    \"n2c2 2014-De-identification\": \"Named Entity Recognition\",\n",
    "    \"IMCS-V2-NER\": \"Named Entity Recognition\",\n",
    "    \"JP-STS\": \"Semantic Similarity\",\n",
    "    \"meddocan\": \"Named Entity Recognition\",\n",
    "    \"MEDIQA 2019-RQE\": \"Natural Language Inference\",\n",
    "    \"MedNLI\": \"Natural Language Inference\",\n",
    "    \"MedSTS\": \"Semantic Similarity\",\n",
    "    \"MTS\": \"Text Classification\",\n",
    "    \"MTS-Temporal\": \"Named Entity Recognition\",\n",
    "    \"n2c2 2018-ADE&medication\": \"Event Extraction\",\n",
    "    \"NorSynthClinical-NER\": \"Named Entity Recognition\",\n",
    "    \"NorSynthClinical-RE\": \"Event Extraction\",\n",
    "    \"NUBES\": \"Event Extraction\",\n",
    "    \"MEDIQA 2023-chat-A\": \"Summarization\",\n",
    "    \"MEDIQA 2023-sum-A\": \"Text Classification\",\n",
    "    \"MEDIQA 2023-sum-B\": \"Summarization\",\n",
    "    \"RuMedDaNet\": \"Natural Language Inference\",\n",
    "    \"CBLUE-CDN\": \"Normalization and Coding\",\n",
    "    \"CHIP-CTC\": \"Text Classification\",\n",
    "    \"CHIP-MDCFNPC\": \"Event Extraction\",\n",
    "    \"MedDG\": \"Question Answering\",\n",
    "    \"IMCS-V2-SR\": \"Event Extraction\",\n",
    "    \"IMCS-V2-MRG\": \"Summarization\",\n",
    "    \"IMCS-V2-DAC\": \"Text Classification\",\n",
    "    \"n2c2 2014-Diabetes\": \"Event Extraction\",\n",
    "    \"n2c2 2014-CAD\": \"Event Extraction\",\n",
    "    \"n2c2 2014-Hyperlipidemia\": \"Event Extraction\",\n",
    "    \"n2c2 2014-Hypertension\": \"Event Extraction\",\n",
    "    \"n2c2 2014-Medication\": \"Event Extraction\",\n",
    "    \"CAS-label\": \"Event Extraction\",\n",
    "    \"CAS-evidence\": \"Summarization\",\n",
    "    \"RuMedNLI\": \"Natural Language Inference\",\n",
    "    \"RuDReC-NER\": \"Named Entity Recognition\",\n",
    "    \"NorSynthClinical-PHI\": \"Named Entity Recognition\",\n",
    "    \"RuCCoN\": \"Named Entity Recognition\",\n",
    "    \"CLISTER\": \"Semantic Similarity\",\n",
    "    \"BRONCO150-NER&Status\": \"Event Extraction\",\n",
    "    \"CARDIO-DE\": \"Named Entity Recognition\",\n",
    "    \"GraSSCo PHI\": \"Named Entity Recognition\",\n",
    "    \"IFMIR-Incident type\": \"Text Classification\",\n",
    "    \"IFMIR-NER\": \"Named Entity Recognition\",\n",
    "    \"IFMIR - NER&factuality\": \"Event Extraction\",\n",
    "    \"iCorpus\": \"Named Entity Recognition\",\n",
    "    \"icliniq-10k\": \"Question Answering\",\n",
    "    \"HealthCareMagic-100k\": \"Question Answering\",\n",
    "    \"MIMIC-IV CDM\": \"Text Classification\",\n",
    "    \"MIMIC-III Outcome.LoS\": \"Text Classification\",\n",
    "    \"MIMIC-III Outcome.Mortality\": \"Text Classification\",\n",
    "    \"MIMIC-IV BHC\": \"Summarization\",\n",
    "    \"MIMIC-IV DiReCT.Dis\": \"Text Classification\",\n",
    "    \"MIMIC-IV DiReCT.PDD\": \"Text Classification\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f2e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_reference(reference, toSort):\n",
    "    # Create a name-to-index mapping from the reference list\n",
    "    order = {name: idx for idx, (name, _) in enumerate(reference)}\n",
    "\n",
    "    # Step 2: Reorder cot_performance using those indices\n",
    "    ans = sorted(toSort, key=lambda x: order.get(x[0], float('inf')))\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e7ab3",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "## 1. Model vs. Performance\n",
    "\n",
    "This creates the first figure in the data analysis Overleaf\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b26811b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sheet_path = \"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet.xlsx\"\n",
    "sheet_path = \"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet.xlsx\"\n",
    "google_sheet = load_sheet(sheet_path)\n",
    "\n",
    "def get_model_performances(google_sheet, sheet_name, cot):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - google_sheet: loaded google sheet (entire .xlsx file)\n",
    "        - sheet_name: name of the sheet you want to load\n",
    "        - cot: boolean indicating whether to get CoT or DA performance\n",
    "    \n",
    "    Output:\n",
    "        - model_performances: Dictionary mapping { model name : list of all task performances (relative difference) with that model}\n",
    "    '''\n",
    "\n",
    "    model_performances = {}\n",
    "\n",
    "    data = google_sheet.parse(sheet_name)\n",
    "\n",
    "    if cot:\n",
    "        col = \"CoT Score\"\n",
    "\n",
    "    else:\n",
    "        col = \"Direct Score\"\n",
    "\n",
    "\n",
    "    for row, model in enumerate(data[\"Model Name\"]):\n",
    "\n",
    "        if model not in model_performances:\n",
    "            model_performances[model] = []\n",
    "        \n",
    "        model_performances[model].append(data[col][row])\n",
    "\n",
    "    return model_performances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5be17bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model_performances = get_model_performances(google_sheet, 'CLF-Difference', cot=True)\n",
    "ext_model_performances = get_model_performances(google_sheet, 'EXT-Difference', cot=True)\n",
    "gen_model_performances = get_model_performances(google_sheet, 'Gen-Difference', cot=True)\n",
    "\n",
    "\n",
    "combined_model_performances = {}\n",
    "\n",
    "# Combine into one\n",
    "for model in clf_model_performances:\n",
    "    combined_model_performances[model] = clf_model_performances[model]\n",
    "    combined_model_performances[model].extend(ext_model_performances[model])\n",
    "    combined_model_performances[model].extend(gen_model_performances[model])\n",
    "\n",
    "# Average performances\n",
    "for model in combined_model_performances:\n",
    "    combined_model_performances[model] = round(sum(combined_model_performances[model]) / len(combined_model_performances[model]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "756d31d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DeepSeek-R1', 42.1), ('gemini-2.0-flash-001', 41.98), ('gpt-4o-0806', 40.66), ('gemini-1.5-pro-002', 40.53), ('Athene-V2-Chat', 39.34), ('DeepSeek-R1-Distill-Llama-70B', 38.95), ('Mistral-Large-Instruct-2411', 38.9), ('Qwen2.5-72B-Instruct', 38.86), ('DeepSeek-R1-Distill-Qwen-32B', 38.72), ('gemma-3-27b-it', 37.55), ('QWQ-32B', 37.03), ('Llama-3.3-70B-Instruct', 36.83), ('Mistral-Small-3.1-24B-Instruct-2503', 36.23), ('gemma-3-12b-it', 35.37), ('Llama-3.1-70B-Instruct', 35.1), ('DeepSeek-R1-Distill-Qwen-14B', 34.79), ('Baichuan-M1-14B-Instruct', 34.36), ('gemma-2-27b-it', 34.22), ('Phi-4', 32.59), ('gpt-35-turbo-0125', 31.63), ('Mistral-Small-24B-Instruct-2501', 31.59), ('Mistral-Small-Instruct-2409', 31.17), ('Qwen2.5-7B-Instruct', 30.25), ('gemma-2-9b-it', 29.94), ('Yi-1.5-34B-Chat-16K', 29.57), ('Llama-3-70B-UltraMedical', 29.44), ('Llama-3.1-8B-Instruct', 29.4), ('Llama-4-Scout-17B-16E-Instruct', 29.38), ('MeLLaMA-70B-chat', 29.25), ('Llama3-OpenBioLLM-70B', 28.78), ('gemma-3-4b-it', 28.19), ('DeepSeek-R1-Distill-Llama-8B', 27.34), ('Ministral-8B-Instruct-2410', 25.91), ('Qwen2.5-3B-Instruct', 25.44), ('Yi-1.5-9B-Chat-16K', 25.4), ('Phi-3.5-MoE-instruct', 25.27), ('Llama-3.1-Nemotron-70B-Instruct-HF', 24.09), ('Phi-3.5-mini-instruct', 23.91), ('DeepSeek-R1-Distill-Qwen-7B', 23.87), ('QwQ-32B-Preview', 23.31), ('Llama-3.2-3B-Instruct', 21.6), ('MeLLaMA-13B-chat', 20.26), ('Qwen2.5-1.5B-Instruct', 19.47), ('Llama-3.1-8B-UltraMedical', 18.34), ('MMed-Llama-3-8B', 16.17), ('gemma-3-1b-it', 13.53), ('DeepSeek-R1-Distill-Qwen-1.5B', 13.42), ('Llama3-OpenBioLLM-8B', 13.29), ('meditron-70b', 13.17), ('Llama-3.2-1B-Instruct', 11.86), ('BioMistral-7B', 10.84), ('meditron-7b', 9.52)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(combined_model_performances.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711cdec",
   "metadata": {},
   "source": [
    "### Get Model Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07598225",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet = pd.ExcelFile(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/Reference for Clinical Benchmark and LLM.xlsx\")\n",
    "\n",
    "model_data = sheet.parse(\"Models (Simplified)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d1f8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size_mapping = {}\n",
    "\n",
    "for row, model_name in enumerate(model_data[\"Name\"]):\n",
    "    if model_name in models:\n",
    "        model_size_mapping[model_name] = model_data[\"Size (B)\"][row]\n",
    "    else:\n",
    "        print(f\"Model {model_name} not found in the list of models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "261b866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DeepSeek-R1-Distill-Qwen-1.5B': 1.5, 'DeepSeek-R1-Distill-Qwen-7B': 7, 'DeepSeek-R1-Distill-Llama-8B': 8, 'DeepSeek-R1-Distill-Qwen-14B': 14, 'DeepSeek-R1-Distill-Qwen-32B': 32, 'DeepSeek-R1-Distill-Llama-70B': 70, 'DeepSeek-R1': 671, 'Baichuan-M1-14B-Instruct': 14, 'gemma-2-9b-it': 9, 'gemma-2-27b-it': 27, 'gemma-3-1b-it': 1, 'gemma-3-4b-it': 4, 'gemma-3-12b-it': 12, 'gemma-3-27b-it': 27, 'Llama-3.1-8B-Instruct': 8, 'Llama-3.1-70B-Instruct': 70, 'Llama-3.2-1B-Instruct': 1, 'Llama-3.2-3B-Instruct': 3, 'Llama-3.3-70B-Instruct': 70, 'Llama-4-Scout-17B-16E-Instruct': 109, 'Llama-3.1-Nemotron-70B-Instruct-HF': 70, 'meditron-7b': 7, 'meditron-70b': 70, 'MeLLaMA-13B-chat': 13, 'MeLLaMA-70B-chat': 70, 'Llama3-OpenBioLLM-8B': 8, 'Llama3-OpenBioLLM-70B': 70, 'MMed-Llama-3-8B': 8, 'Llama-3.1-8B-UltraMedical': 8, 'Llama-3-70B-UltraMedical': 70, 'Ministral-8B-Instruct-2410': 8, 'Mistral-Small-Instruct-2409': 22, 'Mistral-Small-24B-Instruct-2501': 24, 'Mistral-Small-3.1-24B-Instruct-2503': 24, 'Mistral-Large-Instruct-2411': 123, 'BioMistral-7B': 7, 'Phi-3.5-mini-instruct': 4, 'Phi-3.5-MoE-instruct': 42, 'Phi-4': 14, 'Qwen2.5-1.5B-Instruct': 1.5, 'Qwen2.5-3B-Instruct': 3, 'Qwen2.5-7B-Instruct': 7, 'Qwen2.5-72B-Instruct': 72, 'QwQ-32B-Preview': 32, 'QWQ-32B': 32, 'Athene-V2-Chat': 72, 'Yi-1.5-9B-Chat-16K': 9, 'Yi-1.5-34B-Chat-16K': 34, 'gpt-35-turbo-0125': '/', 'gpt-4o-0806': '/', 'gemini-2.0-flash-001': '/', 'gemini-1.5-pro-002': '/'}\n"
     ]
    }
   ],
   "source": [
    "print(model_size_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5686a8",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## 2. NLP Task vs. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1989b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet.xlsx\")\n",
    "\n",
    "def get_nlp_task_performances(google_sheet, sheet_names, is_CoT):\n",
    "    '''\n",
    "    Inputs:\n",
    "    '''\n",
    "\n",
    "    nlp_performances = {}  # maps nlp task --> average performances\n",
    "\n",
    "    for sheet_name in sheet_names:\n",
    "        data = google_sheet.parse(sheet_name)\n",
    "\n",
    "        for row_idx, task_type in enumerate(data['Task Type']):\n",
    "\n",
    "            if task_type not in nlp_performances:\n",
    "                nlp_performances[task_type] = []\n",
    "\n",
    "            if is_CoT:\n",
    "                score = data[\"CoT Score\"][row_idx]\n",
    "            else:\n",
    "                score = data[\"Direct Score\"][row_idx]\n",
    "\n",
    "            nlp_performances[task_type].append(score)\n",
    "\n",
    "    # Average the performances\n",
    "    for nlp_task, values_list in nlp_performances.items():\n",
    "        nlp_performances[nlp_task] = round(sum(values_list) / len(values_list), 2)\n",
    "        \n",
    "    return nlp_performances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0357a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = get_nlp_task_performances(google_sheet, ['CLF-Difference', 'EXT-Difference', 'Gen-Difference'], is_CoT=True)\n",
    "\n",
    "df = pd.DataFrame(perf, index=[0])\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('testingg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbb1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9b813",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "## 3. Types of LLM: Medical vs. General  (Performance Drops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "058dc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet.xlsx\")\n",
    "\n",
    "def get_domain_task_performances(google_sheet, sheet_names, is_CoT, model_domain):\n",
    "    # Want our output to be: \n",
    "    # dictionary mapping { nlp task --> performance on ONLY MEDICAL MODELS }\n",
    "\n",
    "    nlp_performances = {}  # maps nlp task --> average performances of a certain kind of model\n",
    "\n",
    "    for sheet_name in sheet_names:\n",
    "        data = google_sheet.parse(sheet_name)\n",
    "\n",
    "        for row_idx, task_type in enumerate(data['Task Type']):\n",
    "            model_type = data['Model Domain'][row_idx]\n",
    "\n",
    "            # Add the task type to our final dictionary\n",
    "            if task_type not in nlp_performances:\n",
    "                nlp_performances[task_type] = []\n",
    "\n",
    "            # Add the correct metric (direct or CoT) to the dictionary IFF the model type matches the model domain (gen, or med)\n",
    "            if is_CoT and model_type == model_domain:\n",
    "                score = data[\"CoT Score\"][row_idx]\n",
    "                nlp_performances[task_type].append(score)\n",
    "            elif not is_CoT and model_type == model_domain:\n",
    "                score = data[\"Direct Score\"][row_idx]\n",
    "                nlp_performances[task_type].append(score)\n",
    "\n",
    "    # Average the performances\n",
    "    for nlp_task, values_list in nlp_performances.items():\n",
    "        nlp_performances[nlp_task] = round(sum(values_list) / len(values_list), 2)\n",
    "        \n",
    "    # print(nlp_performances)\n",
    "    return nlp_performances\n",
    "\n",
    "#  Need to generate 4 lists\n",
    "#  CoT + gen\n",
    "#  CoT + med\n",
    "#  Direct + gen\n",
    "#  Direct + med\n",
    "\n",
    "sheet_names = ['CLF-Difference', 'EXT-Difference', 'Gen-Difference']\n",
    "\n",
    "cot_general = get_domain_task_performances(google_sheet, sheet_names, is_CoT=True, model_domain=\"gen\")\n",
    "cot_medical = get_domain_task_performances(google_sheet, sheet_names, is_CoT=True, model_domain=\"med\")\n",
    "direct_general = get_domain_task_performances(google_sheet, sheet_names, is_CoT=False, model_domain=\"gen\")\n",
    "direct_medical = get_domain_task_performances(google_sheet, sheet_names, is_CoT=False, model_domain=\"med\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59043219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Text Classification': 56.31, 'Normalization and Coding': 3.38, 'Semantic Similarity': 36.13, 'Natural Language Inference': 76.54, 'Event Extraction': 13.67, 'Named Entity Recognition': 24.79, 'Question Answering': 13.46, 'Summarization': 23.05}\n",
      "{'Text Classification': 42.35, 'Normalization and Coding': 1.31, 'Semantic Similarity': 29.11, 'Natural Language Inference': 63.25, 'Event Extraction': 6.5, 'Named Entity Recognition': 9.46, 'Question Answering': 10.99, 'Summarization': 15.47}\n",
      "{'Text Classification': 58.07, 'Normalization and Coding': 3.69, 'Semantic Similarity': 38.1, 'Natural Language Inference': 77.81, 'Event Extraction': 17.55, 'Named Entity Recognition': 28.37, 'Question Answering': 16.0, 'Summarization': 29.39}\n",
      "{'Text Classification': 45.95, 'Normalization and Coding': 1.54, 'Semantic Similarity': 31.21, 'Natural Language Inference': 67.34, 'Event Extraction': 9.27, 'Named Entity Recognition': 13.02, 'Question Answering': 11.8, 'Summarization': 19.93}\n"
     ]
    }
   ],
   "source": [
    "print(cot_general)\n",
    "print(cot_medical)\n",
    "print(direct_general)\n",
    "print(direct_medical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9642cd4",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## 4. Types of LLM: Commercial vs. Open Source (Performance Drops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a437bf",
   "metadata": {},
   "source": [
    "Very similar to the previous section. This section visualizes how the use of commercial vs. open source models relates to the performance drop associated with using CoT vs. Direct Answering\n",
    "\n",
    "\n",
    "From the previous section, we can conclude that their is a much larger performance drop in medical models using CoT vs. when using DA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e072ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet.xlsx\")\n",
    "\n",
    "def get_domain_task_performances(google_sheet, sheet_names, is_CoT, model_accessibility):\n",
    "    # Want our output to be: \n",
    "    # dictionary mapping { nlp task --> performance on ONLY OPEN SOURCE MODELS }, wlog\n",
    "\n",
    "    nlp_performances = {}  # maps nlp task --> average performances of a certain kind of model\n",
    "\n",
    "    for sheet_name in sheet_names:\n",
    "        data = google_sheet.parse(sheet_name)\n",
    "\n",
    "        for row_idx, task_type in enumerate(data['Task Type']):\n",
    "            model_name = data['Model Name'][row_idx]\n",
    "\n",
    "            model_type = accessibility_mapping[model_name]   # open source or commercial\n",
    "\n",
    "            # # Add the task type to our final dictionary\n",
    "            if task_type not in nlp_performances:\n",
    "                nlp_performances[task_type] = []\n",
    "\n",
    "\n",
    "            if is_CoT and model_type == model_accessibility:   # CoT for commercial/open source:\n",
    "                score = data[\"CoT Score\"][row_idx]\n",
    "                nlp_performances[task_type].append(score)\n",
    "\n",
    "            elif not is_CoT and model_type == model_accessibility:\n",
    "                score = data[\"Direct Score\"][row_idx]\n",
    "                nlp_performances[task_type].append(score)\n",
    "\n",
    "    # Average the performances\n",
    "    for nlp_task, values_list in nlp_performances.items():\n",
    "        nlp_performances[nlp_task] = round(sum(values_list) / len(values_list), 2)\n",
    "        \n",
    "    # print(nlp_performances)\n",
    "    return nlp_performances\n",
    "\n",
    "#  Need to generate 4 lists\n",
    "#  CoT + open\n",
    "#  CoT + commercial\n",
    "#  Direct + open\n",
    "#  Direct + commerical\n",
    "\n",
    "sheet_names = ['CLF-Difference', 'EXT-Difference', 'Gen-Difference']\n",
    "\n",
    "cot_open = get_domain_task_performances(google_sheet, sheet_names, is_CoT=True, model_accessibility=\"open source\")\n",
    "cot_comm = get_domain_task_performances(google_sheet, sheet_names, is_CoT=True, model_accessibility=\"commercial\")\n",
    "direct_open = get_domain_task_performances(google_sheet, sheet_names, is_CoT=False, model_accessibility=\"open source\")\n",
    "direct_comm = get_domain_task_performances(google_sheet, sheet_names, is_CoT=False, model_accessibility=\"commercial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a036baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cot_open)\n",
    "print(cot_comm)\n",
    "print(direct_open)\n",
    "print(direct_comm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5fb039",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## 5. Individual Task vs. Performance\n",
    "\n",
    "\n",
    "Each sheet has different tasks (mutually exclusive). Should iterate through each sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_name_performance(google_sheet, sheet_names, is_CoT):\n",
    "    task_performances = {}  # maps each TASK NAME (i.e. ADE-Identification) to their performances\n",
    "\n",
    "    for sheet_name in sheet_names:\n",
    "\n",
    "        data = google_sheet.parse(sheet_name)\n",
    "\n",
    "        for row_idx, task_name in enumerate(data[\"Task Name\"]):\n",
    "            if task_name not in task_performances:\n",
    "                task_performances[task_name] = []\n",
    "\n",
    "            if is_CoT:\n",
    "                score = data[\"CoT Score\"][row_idx]\n",
    "\n",
    "            else:\n",
    "                score = data[\"Direct Score\"][row_idx]\n",
    "\n",
    "            task_performances[task_name].append(score)\n",
    "\n",
    "    # Average the performances\n",
    "    for task, values_list in task_performances.items():\n",
    "        task_performances[task] = round(sum(values_list) / len(values_list), 2)\n",
    "\n",
    "    return task_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46154cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_names = ['CLF-Difference', 'EXT-Difference', 'Gen-Difference']\n",
    "\n",
    "# Get performances on each task via CoT prompting\n",
    "cot_task_performances = get_task_name_performance(google_sheet, sheet_names, is_CoT=True)\n",
    "\n",
    "# Get performances on each task via Direct answering\n",
    "direct_task_performances = get_task_name_performance(google_sheet, sheet_names, is_CoT=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10319e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cot_task_performances)\n",
    "print(direct_task_performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0120cba",
   "metadata": {},
   "source": [
    "#### Create CSV file for this data\n",
    "\n",
    "\n",
    "- Header (Columns): Task Name, Task Type, Direct Score, CoT Score, Difference, Relative Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b22b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_csv = {\n",
    "    \"Task Name\": [],\n",
    "    \"Task Type\": [],\n",
    "    \"Direct Score\": [],\n",
    "    \"CoT Score\": [],\n",
    "    \"Difference\": [],\n",
    "    \"Relative Difference\": []\n",
    "} \n",
    "\n",
    "for task_name, score in cot_task_performances.items():\n",
    "    final_csv[\"Task Name\"].append(task_name)\n",
    "    final_csv[\"Task Type\"].append(task_to_type_map[task_name])\n",
    "    final_csv[\"CoT Score\"].append(score)\n",
    "\n",
    "for task_name, score in direct_task_performances.items():\n",
    "    final_csv[\"Direct Score\"].append(score)\n",
    "\n",
    "cot_scores = list(cot_task_performances.values())\n",
    "direct_scores = list(direct_task_performances.values())\n",
    "\n",
    "for direct, cot in zip(direct_scores, cot_scores):\n",
    "    final_csv[\"Difference\"].append(round(direct - cot, 2))\n",
    "    final_csv[\"Relative Difference\"].append(round((direct - cot) / direct * 100, 2))\n",
    "\n",
    "df = pd.DataFrame(final_csv)\n",
    "df.to_csv(\"Task Name vs. Performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8dbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_csv)\n",
    "df.to_csv(\"Task Name vs. Performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fde782",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "____\n",
    "____\n",
    "\n",
    "___\n",
    "\n",
    "___\n",
    "\n",
    "### ?. English-Task Only + Model Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a887a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIMIC-III Outcome.Mortality\n",
    "# ADE-Extraction\n",
    "# n2c2 2014-De-identification\n",
    "# MedNLI\n",
    "# HealthCareMagic-100k\n",
    "# MedSTS\n",
    "# MIMIC-IV BHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9399a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tasks = {'MIMIC-III Outcome.Mortality', 'ADE-Extraction', 'n2c2 2014-De-identification', 'MedNLI', 'HealthCareMagic-100k', 'MedSTS', 'MIMIC-IV BHC'}\n",
    "\n",
    "models = {'Qwen2.5-72B-Instruct', 'Mistral-Large-Instruct-2411', 'Athene-V2-Chat', 'Llama-3.3-70B-Instruct'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7545dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet.xlsx\")\n",
    "\n",
    "data = google_sheet.parse('All-Sheet')\n",
    "\n",
    "\n",
    "def get_scores(google_sheet, isCoT):\n",
    "    result = {}  # mapping of task: [list of scores]\n",
    "\n",
    "    for row_idx, task in enumerate(data[\"Task Name\"]):\n",
    "        if task in english_tasks:\n",
    "\n",
    "            if task not in result:\n",
    "                result[task] = []\n",
    "\n",
    "            model_name = data[\"Model Name\"][row_idx]\n",
    "\n",
    "            if model_name in models:\n",
    "                if isCoT:\n",
    "                    result[task].append(data[\"CoT Score\"][row_idx])\n",
    "                else:\n",
    "                    result[task].append(data[\"Direct Score\"][row_idx])\n",
    "\n",
    "        \n",
    "    return result\n",
    "\n",
    "cot = get_scores(google_sheet, True)\n",
    "direct = get_scores(google_sheet, False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83887288",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cot)\n",
    "print(direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413a21a",
   "metadata": {},
   "source": [
    "#### Compute Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1525bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Average values for each task\n",
    "# for key, values_ls in cot.items():\n",
    "#     cot[key] = round(sum(values_ls) / len(values_ls), 2)\n",
    "\n",
    "# for key, values_ls in direct.items():\n",
    "#     direct[key] = round(sum(values_ls) / len(values_ls), 2)\n",
    "\n",
    "print('CoT:', cot)\n",
    "print('Direct:', direct)\n",
    "\n",
    "# Compute relative differences (%) for each task   --> (direct - cot) / direct\n",
    "relative_differences = {}\n",
    "for key in cot:\n",
    "    relative_diff = round( ((direct[key] - cot[key]) / direct[key]) * 100, 2)\n",
    "\n",
    "    relative_differences[key] = relative_diff\n",
    "\n",
    "print(relative_differences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c94e0b",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "___\n",
    "\n",
    "## 6. High-Low LLM vs. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e33bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models(google_sheet):\n",
    "    data = google_sheet.parse(\"All-Sheet\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for row_idx, model_name in enumerate(data[\"Model Name\"]):\n",
    "\n",
    "        if model_name not in results:\n",
    "            results[model_name] = []\n",
    "\n",
    "        direct_score = data[\"Direct Score\"][row_idx]\n",
    "        cot_score = data[\"CoT Score\"][row_idx]\n",
    "\n",
    "        results[model_name].append(float(direct_score))\n",
    "        results[model_name].append(float(cot_score))\n",
    "\n",
    "    # Average Performances\n",
    "    for model_name in results:\n",
    "        results[model_name] = round( sum(results[model_name]) / len(results[model_name]), 2)\n",
    "\n",
    "    return results\n",
    "\n",
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet.xlsx\")\n",
    "\n",
    "results = get_best_models(google_sheet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb2b6a",
   "metadata": {},
   "source": [
    "#### Sort the results by average performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4555cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_models = sorted(results.items(), key = lambda x: x[1], reverse=True )\n",
    "print(sorted_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f8b67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Qwen2.5-1.5B-Instruct', 'MeLLaMA-13B-chat', 'Llama-3.1-8B-UltraMedical', 'MMed-Llama-3-8B', 'BioMistral-7B', 'gemma-3-1b-it', 'meditron-70b', 'DeepSeek-R1-Distill-Qwen-1.5B', 'Llama3-OpenBioLLM-8B', 'Llama-3.2-1B-Instruct', 'meditron-7b']\n"
     ]
    }
   ],
   "source": [
    "top_20 = [('DeepSeek-R1', 43.17), ('gemini-2.0-flash-001', 42.51), ('gpt-4o-0806', 42.43), ('gemini-1.5-pro-002', 42.19), ('Mistral-Large-Instruct-2411', 40.59), ('Athene-V2-Chat', 40.52), ('Qwen2.5-72B-Instruct', 40.24), ('DeepSeek-R1-Distill-Llama-70B', 39.37), ('DeepSeek-R1-Distill-Qwen-32B', 39.23), ('gemma-3-27b-it', 38.72)]\n",
    "top_20_40 = [('Llama-3.3-70B-Instruct', 38.35), ('QWQ-32B', 38.2), ('Mistral-Small-3.1-24B-Instruct-2503', 37.98), ('Llama-3.1-70B-Instruct', 37.1), ('gemma-3-12b-it', 36.35), ('gemma-2-27b-it', 36.22), ('Baichuan-M1-14B-Instruct', 35.22), ('Mistral-Small-24B-Instruct-2501', 34.56), ('DeepSeek-R1-Distill-Qwen-14B', 34.54), ('Phi-4', 34.36)]\n",
    "top_40_60 = [('gpt-35-turbo-0125', 33.46), ('Mistral-Small-Instruct-2409', 33.18), ('gemma-2-9b-it', 32.51), ('Llama-4-Scout-17B-16E-Instruct', 32.25), ('Llama-3-70B-UltraMedical', 31.42), ('Llama3-OpenBioLLM-70B', 30.9), ('Yi-1.5-34B-Chat-16K', 30.85), ('Qwen2.5-7B-Instruct', 30.78), ('MeLLaMA-70B-chat', 30.76), ('Llama-3.1-8B-Instruct', 29.19)]\n",
    "top_60_80 = [('Llama-3.1-Nemotron-70B-Instruct-HF', 28.42), ('gemma-3-4b-it', 28.38), ('Ministral-8B-Instruct-2410', 28.14), ('DeepSeek-R1-Distill-Llama-8B', 27.91), ('QwQ-32B-Preview', 27.53), ('Phi-3.5-MoE-instruct', 27.41), ('Yi-1.5-9B-Chat-16K', 27.1), ('Qwen2.5-3B-Instruct', 26.01), ('Phi-3.5-mini-instruct', 24.66), ('DeepSeek-R1-Distill-Qwen-7B', 24.57), ('Llama-3.2-3B-Instruct', 22.25)]\n",
    "bottom_20 = [('Qwen2.5-1.5B-Instruct', 20.82), ('MeLLaMA-13B-chat', 20.51), ('Llama-3.1-8B-UltraMedical', 19.25), ('MMed-Llama-3-8B', 18.27), ('BioMistral-7B', 15.63), ('gemma-3-1b-it', 14.63), ('meditron-70b', 14.43), ('DeepSeek-R1-Distill-Qwen-1.5B', 13.84), ('Llama3-OpenBioLLM-8B', 13.74), ('Llama-3.2-1B-Instruct', 12.29), ('meditron-7b', 9.52)]\n",
    "\n",
    "# print(len(top_20))\n",
    "# print(len(top_20_40))\n",
    "# print(len(top_40_60))\n",
    "# print(len(top_60_80))\n",
    "# print(len(bottom_20))\n",
    "\n",
    "top_20 = [tp[0] for tp in top_20]\n",
    "top_20_40 = [tp[0] for tp in top_20_40]\n",
    "top_40_60 = [tp[0] for tp in top_40_60]\n",
    "top_60_80 = [tp[0] for tp in top_60_80]\n",
    "bottom_20 = [tp[0] for tp in bottom_20]\n",
    "\n",
    "print(bottom_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea93de6",
   "metadata": {},
   "source": [
    "#### Get the average performances for tasks evaluated with each group of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca2ebeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.42\n",
      "7.81\n",
      "9.95\n",
      "11.62\n",
      "14.07\n"
     ]
    }
   ],
   "source": [
    "def get_average_performances(google_sheet, valid_models):\n",
    "    '''\n",
    "    valid_models: list containing the __ 20% of models\n",
    "    '''\n",
    "    data = google_sheet.parse(\"All-Sheet\")\n",
    "    \n",
    "    cot_scores = []\n",
    "    direct_scores = []\n",
    "\n",
    "    # want to determien the relationshpi between the average performance of a gorup of models AND how CoT/direct impacts\n",
    "    # Average the performance of all tasks with \n",
    "    for row_idx, model_name in enumerate(data[\"Model Name\"]):\n",
    "        # Must be in the valid models\n",
    "        if model_name in valid_models:\n",
    "            cot_scores.append(data[\"CoT Score\"][row_idx])\n",
    "            direct_scores.append(data[\"Direct Score\"][row_idx])\n",
    "\n",
    "    avg_cot_perf = round(sum(cot_scores) / len(cot_scores), 2)\n",
    "    avg_direct_perf = round(sum(direct_scores) / len(direct_scores), 2)\n",
    "\n",
    "    # Relative difference\n",
    "    relative_diff = round(((avg_direct_perf - avg_cot_perf) / avg_direct_perf) * 100, 2)\n",
    "\n",
    "    return relative_diff\n",
    "\n",
    "top_20_diff = get_average_performances(google_sheet, top_20)\n",
    "top_40_diff = get_average_performances(google_sheet, top_20_40)\n",
    "top_60_diff = get_average_performances(google_sheet, top_40_60)\n",
    "top_80_diff = get_average_performances(google_sheet, top_60_80)\n",
    "bottom_20_diff = get_average_performances(google_sheet, bottom_20)\n",
    "\n",
    "print(top_20_diff)\n",
    "print(top_40_diff)\n",
    "print(top_60_diff)\n",
    "print(top_80_diff)\n",
    "print(bottom_20_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5405d9",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "\n",
    "### 6.2 Top LLMs. vs. Invalid Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80da07b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6281995133819953\n",
      "2.58007722007722\n",
      "1.1230293159609122\n",
      "4.091622023809523\n",
      "13.103409669211194\n"
     ]
    }
   ],
   "source": [
    "def get_average_performances(google_sheet, valid_models):\n",
    "    '''\n",
    "    valid_models: list containing the __ 20% of models\n",
    "    '''\n",
    "    data = google_sheet.parse(\"All-Sheet\")\n",
    "    \n",
    "    invalid_scores = []\n",
    "\n",
    "    # want to determien the relationshpi between the average performance of a gorup of models AND how CoT/direct impacts\n",
    "    # Average the performance of all tasks with \n",
    "    for row_idx, model_name in enumerate(data[\"Model Name\"]):\n",
    "    \n",
    "        # Must be in the valid models\n",
    "        if model_name in valid_models:\n",
    "            if data[\"Invalid Difference\"][row_idx] == 0:\n",
    "                continue\n",
    "            invalid_scores.append(data[\"Invalid Difference\"][row_idx])\n",
    "\n",
    "   \n",
    "    avg_invalid_diff = np.mean(invalid_scores)\n",
    "\n",
    "    return avg_invalid_diff\n",
    "\n",
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet-With-Invalid.xlsx\")\n",
    "top_20_diff = get_average_performances(google_sheet, top_20)\n",
    "top_40_diff = get_average_performances(google_sheet, top_20_40)\n",
    "top_60_diff = get_average_performances(google_sheet, top_40_60)\n",
    "top_80_diff = get_average_performances(google_sheet, top_60_80)\n",
    "bottom_20_diff = get_average_performances(google_sheet, bottom_20)\n",
    "\n",
    "print(top_20_diff)\n",
    "print(top_40_diff)\n",
    "print(top_60_diff)\n",
    "print(top_80_diff)\n",
    "print(bottom_20_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5975f567",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0a199",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## 7. English-Only Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb4f6e",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "#### ALL ENGLISH TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39020a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_english_tasks = {\n",
    "    \"ADE-Identification\",\n",
    "    \"ADE-Extraction\",\n",
    "    \"ADE-Drug dosage\",\n",
    "    \"BrainMRI-AIS\",\n",
    "    \"ClinicalNotes-UPMC\",\n",
    "    \"CLIP\",\n",
    "    \"GOUT-CC-Consensus\",\n",
    "    \"n2c2 2006-De-identification\",\n",
    "    \"Medication extraction\",\n",
    "    \"n2c2 2010-Concept\",\n",
    "    \"n2c2 2010-Assertion\",\n",
    "    \"n2c2 2010-Relation\",\n",
    "    \"n2c2 2014-De-identification\",\n",
    "    \"MEDIQA 2019-RQE\",\n",
    "    \"MedNLI\",\n",
    "    \"MedSTS\",\n",
    "    \"MTS\",\n",
    "    \"MTS-Temporal\",\n",
    "    \"n2c2 2018-ADE&medication\",\n",
    "    \"MEDIQA 2023-chat-A\",\n",
    "    \"MEDIQA 2023-sum-A\",\n",
    "    \"MEDIQA 2023-sum-B\",\n",
    "    \"n2c2 2014-Diabetes\",\n",
    "    \"n2c2 2014-CAD\",\n",
    "    \"n2c2 2014-Hyperlipidemia\",\n",
    "    \"n2c2 2014-Hypertension\",\n",
    "    \"n2c2 2014-Medication\",\n",
    "    \"icliniq-10k\",\n",
    "    \"HealthCareMagic-100k\",\n",
    "    \"MIMIC-IV CDM\",\n",
    "    \"MIMIC-III Outcome.LoS\",\n",
    "    \"MIMIC-III Outcome.Mortality\",\n",
    "    \"MIMIC-IV BHC\",\n",
    "    \"MIMIC-IV DiReCT.Dis\",\n",
    "    \"MIMIC-IV DiReCT.PDD\"\n",
    "}\n",
    "\n",
    "print(len(all_english_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e6575",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50117c",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### 7.1 Overall Performance of LLMs Across English Tasks and Different Inference Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16abcd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performance(google_sheet, isCoT):\n",
    "    data = google_sheet.parse(\"All-Sheet\")\n",
    "\n",
    "    results = {}   # maps model --> performances iff task is english\n",
    "\n",
    "    for row_idx, model_name in enumerate(data[\"Model Name\"]):\n",
    "        if model_name not in results:\n",
    "            results[model_name] = []\n",
    "\n",
    "        task = data[\"Task Name\"][row_idx]\n",
    "\n",
    "        if task in all_english_tasks:\n",
    "            if isCoT:\n",
    "                cot_score = data[\"CoT Score\"][row_idx]\n",
    "                results[model_name].append(cot_score)\n",
    "\n",
    "            else:\n",
    "                direct_score = data[\"Direct Score\"][row_idx]\n",
    "                results[model_name].append(direct_score)\n",
    "\n",
    "    for key in results:\n",
    "        results[key] = round(np.mean(results[key]), 2)\n",
    "\n",
    "    return sorted(results.items(), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "direct_performances = get_model_performance(google_sheet, False)\n",
    "cot_performances = get_model_performance(google_sheet, True)\n",
    "\n",
    "\n",
    "\n",
    "# SORT COT TO HAVE THE SAME ORDER\n",
    "\n",
    "# Create a name-to-index mapping from the reference list\n",
    "order = {name: idx for idx, (name, _) in enumerate(direct_performances)}\n",
    "\n",
    "# Step 2: Reorder cot_performance using those indices\n",
    "sorted_cot = sorted(cot_performances, key=lambda x: order.get(x[0], float('inf')))\n",
    "\n",
    "\n",
    "# Sorted by direct scores\n",
    "print(direct_performances)\n",
    "print(sorted_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1ebb87",
   "metadata": {},
   "source": [
    "#### Construct CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {\n",
    "    \"Model\": [],\n",
    "    \"Model Size\": [],\n",
    "    \"Model Type\": [],\n",
    "    \"Model Domain\": [],\n",
    "    \"Direct Score\": [],\n",
    "    \"CoT Score\": [],\n",
    "    \"Difference\": [],\n",
    "    \"Relative Difference\": []\n",
    "}\n",
    "\n",
    "\n",
    "# Add Direct and Cot Scores\n",
    "for model, score in direct_performances:\n",
    "    table[\"Model\"].append(model)\n",
    "    table[\"Model Size\"].append(model_size_mapping[model])\n",
    "    table[\"Model Type\"].append(accessibility_mapping[model])\n",
    "    table[\"Model Domain\"].append(model_domain_mapping[model])\n",
    "    table[\"Direct Score\"].append(score)\n",
    "\n",
    "for model, score in sorted_cot:\n",
    "    table[\"CoT Score\"].append(score)\n",
    "\n",
    "# Fix Accessibility stuff\n",
    "for i, access in enumerate(table[\"Model Type\"]):\n",
    "    if access == 'commercial':\n",
    "        table[\"Model Type\"][i] = \"Commercial\"\n",
    "\n",
    "    elif access == \"open source\":\n",
    "        table[\"Model Type\"][i] = \"Open-Source\"\n",
    "        \n",
    "\n",
    "# Calculate Differences!\n",
    "\n",
    "for idx, direct_score in enumerate(table[\"Direct Score\"]):\n",
    "    diff = round(table[\"CoT Score\"][idx] - direct_score, 2)\n",
    "\n",
    "    table['Difference'].append(diff)\n",
    "\n",
    "    relative_diff = round( (diff / direct_score) * 100, 2 )\n",
    "\n",
    "    table[\"Relative Difference\"].append(relative_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb930a3",
   "metadata": {},
   "source": [
    "#### CREATE CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table)\n",
    "df.to_csv(\"English-LLM-Performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda5f36",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff24548",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "### 7.2 NLP Task vs. Performance for English Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae222509",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet.xlsx\")\n",
    "\n",
    "def get_nlp_task_performances(google_sheet, sheet_names, is_CoT):\n",
    "    '''\n",
    "    Inputs:\n",
    "    '''\n",
    "\n",
    "    nlp_performances = {}  # maps nlp task --> average performances\n",
    "\n",
    "    # for sheet_name in sheet_names:\n",
    "    counter = 0\n",
    "    data = google_sheet.parse(\"All-Sheet\")\n",
    "\n",
    "    for row_idx, task_type in enumerate(data['Task Type']):\n",
    "        task_name = data[\"Task Name\"][row_idx]\n",
    "        if task_name in all_english_tasks:\n",
    "            if task_type not in nlp_performances:\n",
    "                nlp_performances[task_type] = []\n",
    "\n",
    "            if is_CoT:\n",
    "                score = data[\"CoT Score\"][row_idx]\n",
    "            else:\n",
    "                score = data[\"Direct Score\"][row_idx]\n",
    "\n",
    "            nlp_performances[task_type].append(score)\n",
    "\n",
    "\n",
    "    # Average the performances\n",
    "    for nlp_task, values_list in nlp_performances.items():\n",
    "        nlp_performances[nlp_task] = round(sum(values_list) / len(values_list), 2)\n",
    "        # nlp_performances[nlp_task] = round(np.mean(values_list))\n",
    "\n",
    "    return nlp_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef16c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_perf = get_nlp_task_performances(google_sheet, ['CLF-Difference', 'EXT-Difference', 'Gen-Difference'], is_CoT=True)\n",
    "direct_perf = get_nlp_task_performances(google_sheet, ['CLF-Difference', 'EXT-Difference', 'Gen-Difference'], is_CoT=False)\n",
    "\n",
    "\n",
    "direct_perf = sorted(direct_perf.items(), key = lambda x: x[1], reverse=True)\n",
    "cot_perf = sorted(cot_perf.items(), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "cor_perf = sort_by_reference(direct_perf, cot_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8300766",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(direct_perf)\n",
    "print(cot_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0fe76",
   "metadata": {},
   "source": [
    "#### Create CSV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69285e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {\n",
    "    \"NLP Task\": [],\n",
    "    \"Task Type\": [],\n",
    "    \"Direct Score\": [],\n",
    "    \"CoT Score\": [],\n",
    "    \"Difference\": [],\n",
    "    \"Relative Difference\": []\n",
    "}\n",
    "\n",
    "for task, score in direct_perf:\n",
    "    table[\"NLP Task\"].append(task)\n",
    "    table[\"Task Type\"].append('1')\n",
    "    table[\"Direct Score\"].append(score)\n",
    "\n",
    "for task, score in cot_perf:\n",
    "    table['CoT Score'].append(score)\n",
    "\n",
    "for idx, direct_score in enumerate(table[\"Direct Score\"]):\n",
    "    diff = round( table[\"CoT Score\"][idx] - direct_score, 2)\n",
    "\n",
    "    table['Difference'].append(diff)\n",
    "\n",
    "    relative_diff = round( (diff / direct_score) * 100, 2 )\n",
    "\n",
    "    table[\"Relative Difference\"].append(relative_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13922f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table)\n",
    "df.to_csv(\"English-NLP-Task.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71d12e",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "___\n",
    "\n",
    "#### 7.3 English-Only: Model Domain (Medical vs. General)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet.xlsx\")\n",
    "\n",
    "def get_domain_task_performances(google_sheet, is_CoT, model_domain):\n",
    "    # Want our output to be: \n",
    "    # dictionary mapping { nlp task --> performance on ONLY MEDICAL MODELS }\n",
    "\n",
    "    nlp_performances = {}  # maps nlp task --> average performances of a certain kind of model\n",
    "\n",
    "    data = google_sheet.parse(\"All-Sheet\")\n",
    "\n",
    "    for row_idx, task_type in enumerate(data['Task Type']):\n",
    "        task_name = data[\"Task Name\"][row_idx]\n",
    "        if task_name in all_english_tasks:\n",
    "            model_type = data['Model Domain'][row_idx]\n",
    "            # Add the task type to our final dictionary\n",
    "            if task_type not in nlp_performances:\n",
    "                nlp_performances[task_type] = []\n",
    "\n",
    "            # Add the correct metric (direct or CoT) to the dictionary IFF the model type matches the model domain (gen, or med)\n",
    "            if is_CoT and model_type == model_domain:\n",
    "                score = data[\"CoT Score\"][row_idx]\n",
    "                nlp_performances[task_type].append(score)\n",
    "\n",
    "            elif not is_CoT and model_type == model_domain:\n",
    "                score = data[\"Direct Score\"][row_idx]\n",
    "                nlp_performances[task_type].append(score)\n",
    "\n",
    "    # Average the performances\n",
    "    for nlp_task, values_list in nlp_performances.items():\n",
    "        nlp_performances[nlp_task] = round(sum(values_list) / len(values_list), 2)\n",
    "        \n",
    "    # print(nlp_performances)\n",
    "    return nlp_performances\n",
    "\n",
    "#  Need to generate 4 lists\n",
    "#  CoT + gen\n",
    "#  CoT + med\n",
    "#  Direct + gen\n",
    "#  Direct + med\n",
    "\n",
    "sheet_names = ['CLF-Difference', 'EXT-Difference', 'Gen-Difference']\n",
    "\n",
    "cot_general = get_domain_task_performances(google_sheet, is_CoT=True, model_domain=\"gen\")\n",
    "cot_medical = get_domain_task_performances(google_sheet, is_CoT=True, model_domain=\"med\")\n",
    "direct_general = get_domain_task_performances(google_sheet, is_CoT=False, model_domain=\"gen\")\n",
    "direct_medical = get_domain_task_performances(google_sheet, is_CoT=False, model_domain=\"med\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fbf9d8",
   "metadata": {},
   "source": [
    "### Create CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cot_general)\n",
    "print(cot_medical)\n",
    "print(direct_general)\n",
    "print(direct_medical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {\n",
    "    \"NLP Task\": [],\n",
    "    \"D, Gen\": [],\n",
    "    \"CoT, Gen\": [],\n",
    "    \"D, Med\": [],\n",
    "    \"CoT, Med\": [],\n",
    "    \"Gen Diff\": [],\n",
    "    \"Med Diff\": [],\n",
    "    \"Gen Relative Diff\": [],\n",
    "    \"Med Relative Diff\": []\n",
    "}\n",
    "\n",
    "for task, score in cot_general.items():\n",
    "    table[\"NLP Task\"].append(task)\n",
    "    table[\"CoT, Gen\"].append(score)\n",
    "\n",
    "for task, score in cot_medical.items():\n",
    "    table[\"CoT, Med\"].append(score)\n",
    "\n",
    "for task, score in direct_general.items():\n",
    "    table[\"D, Gen\"].append(score)\n",
    "\n",
    "for task, score in direct_medical.items():\n",
    "    table[\"D, Med\"].append(score)\n",
    "\n",
    "\n",
    "for i, score in enumerate(table[\"D, Gen\"]):\n",
    "    gen_diff = round(table[\"CoT, Gen\"][i] - table[\"D, Gen\"][i], 2)\n",
    "    med_diff = round(table[\"CoT, Med\"][i] - table[\"D, Med\"][i], 2)\n",
    "\n",
    "    table[\"Gen Diff\"].append(gen_diff)\n",
    "    table[\"Med Diff\"].append(med_diff)\n",
    "\n",
    "    gen_rel_diff = round( (gen_diff / table[\"D, Gen\"][i]) * 100, 2 )\n",
    "    med_rel_diff = round( (med_diff / table[\"D, Med\"][i]) * 100, 2 )\n",
    "\n",
    "    table[\"Gen Relative Diff\"].append(gen_rel_diff)\n",
    "    table[\"Med Relative Diff\"].append(med_rel_diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table)\n",
    "df.to_csv(\"English LLM Domain vs. Performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213164f",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "___\n",
    "\n",
    "### 7. 4 English Only: Commercial vs. Open Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4315893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet.xlsx\")\n",
    "\n",
    "def get_domain_task_performances(google_sheet, is_CoT, model_accessibility):\n",
    "    # Want our output to be: \n",
    "    # dictionary mapping { nlp task --> performance on ONLY OPEN SOURCE MODELS }, wlog\n",
    "\n",
    "    nlp_performances = {}  # maps nlp task --> average performances of a certain kind of model\n",
    "\n",
    "\n",
    "    data = google_sheet.parse(\"All-Sheet\")\n",
    "\n",
    "    for row_idx, task_type in enumerate(data['Task Type']):\n",
    "\n",
    "        task_name = data[\"Task Name\"][row_idx]\n",
    "\n",
    "        if task_name in all_english_tasks:\n",
    "            model_name = data['Model Name'][row_idx]\n",
    "\n",
    "            model_type = accessibility_mapping[model_name]   # open source or commercial\n",
    "\n",
    "            # # Add the task type to our final dictionary\n",
    "            if task_type not in nlp_performances:\n",
    "                nlp_performances[task_type] = []\n",
    "\n",
    "\n",
    "            if is_CoT and model_type == model_accessibility:   # CoT for commercial/open source:\n",
    "                score = data[\"CoT Score\"][row_idx]\n",
    "                nlp_performances[task_type].append(score)\n",
    "\n",
    "            elif not is_CoT and model_type == model_accessibility:\n",
    "                score = data[\"Direct Score\"][row_idx]\n",
    "                nlp_performances[task_type].append(score)\n",
    "\n",
    "    # Average the performances\n",
    "    for nlp_task, values_list in nlp_performances.items():\n",
    "        nlp_performances[nlp_task] = round(sum(values_list) / len(values_list), 2)\n",
    "        \n",
    "    # print(nlp_performances)\n",
    "    return nlp_performances\n",
    "\n",
    "#  Need to generate 4 lists\n",
    "#  CoT + open\n",
    "#  CoT + commercial\n",
    "#  Direct + open\n",
    "#  Direct + commerical\n",
    "\n",
    "sheet_names = ['CLF-Difference', 'EXT-Difference', 'Gen-Difference']\n",
    "\n",
    "cot_open = get_domain_task_performances(google_sheet, is_CoT=True, model_accessibility=\"open source\")\n",
    "cot_comm = get_domain_task_performances(google_sheet, is_CoT=True, model_accessibility=\"commercial\")\n",
    "direct_open = get_domain_task_performances(google_sheet, is_CoT=False, model_accessibility=\"open source\")\n",
    "direct_comm = get_domain_task_performances(google_sheet, is_CoT=False, model_accessibility=\"commercial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {\n",
    "    \"NLP Task\": [],\n",
    "    \"Dir, Open\": [],\n",
    "    \"CoT, Open\": [],\n",
    "    \"Dir, Comm\": [],\n",
    "    \"CoT, Comm\": [],\n",
    "    \"Open Diff\": [],\n",
    "    \"Comm Diff\": [],\n",
    "    \"Open Relative Diff\": [],\n",
    "    \"Comm Relative Diff\": []\n",
    "}\n",
    "\n",
    "for task, score in cot_open.items():\n",
    "    table[\"NLP Task\"].append(task)\n",
    "    table[\"CoT, Open\"].append(score)\n",
    "\n",
    "for task, score in cot_comm.items():\n",
    "    table[\"CoT, Comm\"].append(score)\n",
    "\n",
    "for task, score in direct_open.items():\n",
    "    table[\"Dir, Open\"].append(score)\n",
    "\n",
    "for task, score in direct_comm.items():\n",
    "    table[\"Dir, Comm\"].append(score)\n",
    "\n",
    "\n",
    "for i, score in enumerate(table[\"Dir, Open\"]):\n",
    "    open_diff = round(table[\"CoT, Open\"][i] - table[\"Dir, Open\"][i], 2)\n",
    "    comm_diff = round(table[\"CoT, Comm\"][i] - table[\"Dir, Comm\"][i], 2)\n",
    "\n",
    "    table[\"Open Diff\"].append(open_diff)\n",
    "    table[\"Comm Diff\"].append(comm_diff)\n",
    "\n",
    "    open_rel_diff = round( (open_diff / table[\"Dir, Open\"][i]) * 100, 2 )\n",
    "    comm_rel_diff = round( (comm_diff / table[\"Dir, Comm\"][i]) * 100, 2 )\n",
    "\n",
    "    table[\"Open Relative Diff\"].append(open_rel_diff)\n",
    "    table[\"Comm Relative Diff\"].append(comm_rel_diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dace51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table)\n",
    "df.to_csv(\"English LLM Domain vs. Accessibility.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8bba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_name_performance(google_sheet, is_CoT):\n",
    "    task_performances = {}  # maps each TASK NAME (i.e. ADE-Identification) to their performances\n",
    "\n",
    "    data = google_sheet.parse(\"All-Sheet\")\n",
    "\n",
    "    for row_idx, task_name in enumerate(data[\"Task Name\"]):\n",
    "        if task_name in all_english_tasks:\n",
    "            if task_name not in task_performances:\n",
    "                task_performances[task_name] = []\n",
    "\n",
    "            if is_CoT:\n",
    "                score = data[\"CoT Score\"][row_idx]\n",
    "\n",
    "            else:\n",
    "                score = data[\"Direct Score\"][row_idx]\n",
    "\n",
    "            task_performances[task_name].append(score)\n",
    "\n",
    "    # Average the performances\n",
    "    for task, values_list in task_performances.items():\n",
    "        task_performances[task] = round(sum(values_list) / len(values_list), 2)\n",
    "\n",
    "    return task_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec201e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performances on each task via CoT prompting\n",
    "cot_task_performances = get_task_name_performance(google_sheet, is_CoT=True)\n",
    "\n",
    "# Get performances on each task via Direct answering\n",
    "direct_task_performances = get_task_name_performance(google_sheet, is_CoT=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_csv = {\n",
    "    \"Task Name\": [],\n",
    "    \"Task Type\": [],\n",
    "    \"Direct Score\": [],\n",
    "    \"CoT Score\": [],\n",
    "    \"Difference\": [],\n",
    "    \"Relative Difference\": []\n",
    "} \n",
    "\n",
    "for task_name, score in cot_task_performances.items():\n",
    "    final_csv[\"Task Name\"].append(task_name)\n",
    "    final_csv[\"Task Type\"].append(task_to_type_map[task_name])\n",
    "    final_csv[\"CoT Score\"].append(score)\n",
    "\n",
    "for task_name, score in direct_task_performances.items():\n",
    "    final_csv[\"Direct Score\"].append(score)\n",
    "\n",
    "cot_scores = list(cot_task_performances.values())\n",
    "direct_scores = list(direct_task_performances.values())\n",
    "\n",
    "for direct, cot in zip(direct_scores, cot_scores):\n",
    "    final_csv[\"Difference\"].append(round(direct - cot, 2))\n",
    "    final_csv[\"Relative Difference\"].append(round((direct - cot) / direct * 100, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7812ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_csv)\n",
    "df.to_csv(\"English Task Name vs. Performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744bc29e",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 8. Length of CoT Output --> SEE \"word_count_analysis.ipynb\"\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fac203",
   "metadata": {},
   "source": [
    "## 9. Multi-Prompt Analysis (see other file \"prompt_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d87711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24552ecf",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 10. Exclude Invalid Tasks\n",
    "\n",
    "A follow-up to section 6, where we only evaluate and analyze the performance drops for tasks the have NO INVALID score for both CoT and Direct. We compare the performance drops across different model groups to see whether the drops in performance are due to invalid rates, or the models other features as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea85109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sheet with only tasks that have 0 invalid rate on both CoT and Direct\n",
    "# We already have the model splits (by 20% in performance)\n",
    "# Get drops in performance for each group\n",
    "# Issue is that different models have different tasks upon which they got 0 invalid rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c66630c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03239651416122004\n",
      "-0.06096590909090909\n",
      "0.11828125\n",
      "0.1392982456140351\n",
      "0.023976608187134492\n"
     ]
    }
   ],
   "source": [
    "def get_performances_no_invalid(google_sheet, valid_models):\n",
    "    '''\n",
    "    valid_models: list containing the __ 20% of models\n",
    "    '''\n",
    "    data = google_sheet.parse(\"No-Invalid-Task-Sheet\")\n",
    "    \n",
    "    relative_diff = []\n",
    "\n",
    "    # want to determien the relationshpi between the average performance of a gorup of models AND how CoT/direct impacts\n",
    "    # Average the performance of all tasks with \n",
    "    for row_idx, model_name in enumerate(data[\"Model Name\"]):\n",
    "    \n",
    "        # Must be in the valid models\n",
    "        if model_name in valid_models:\n",
    "            relative_diff.append(data[\"Relative Difference\"][row_idx])\n",
    "\n",
    "   \n",
    "    avg_relative_diff = np.mean(relative_diff)\n",
    "\n",
    "\n",
    "    return avg_relative_diff\n",
    "\n",
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet-With-Invalid.xlsx\")\n",
    "\n",
    "top_20_diff = get_performances_no_invalid(google_sheet, top_20)\n",
    "top_40_diff = get_performances_no_invalid(google_sheet, top_20_40)\n",
    "top_60_diff = get_performances_no_invalid(google_sheet, top_40_60)\n",
    "top_80_diff = get_performances_no_invalid(google_sheet, top_60_80)\n",
    "bottom_20_diff = get_performances_no_invalid(google_sheet, bottom_20)\n",
    "\n",
    "print(top_20_diff)\n",
    "print(top_40_diff)\n",
    "print(top_60_diff)\n",
    "print(top_80_diff)\n",
    "print(bottom_20_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41a6b5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(bottom_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01ca938a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(top_20_40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bb61b",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 11. Revised CoT Output Length Analysis\n",
    "\n",
    "See word_count_analysis_2.0\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1470be",
   "metadata": {},
   "source": [
    "## 12. Group Models by Model Size\n",
    "\n",
    "Similar to part 6, but we bucket by model sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4317b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_model_sizes(google_sheet):\n",
    "    data = google_sheet.parse(\"All-Sheet\")\n",
    "\n",
    "    no = set()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for row_idx, model_name in enumerate(data[\"Model Name\"]):\n",
    "        if np.isnan(data[\"Model Size\"][row_idx]):\n",
    "            no.add(model_name)\n",
    "            continue\n",
    "        \n",
    "        if model_name not in results:\n",
    "            results[model_name] = []\n",
    "\n",
    "        results[model_name] = data[\"Model Size\"][row_idx]\n",
    "\n",
    "    return results\n",
    "\n",
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet.xlsx\")\n",
    "\n",
    "results = get_model_sizes(google_sheet)\n",
    "\n",
    "sorted_models = sorted(results.items(), key = lambda x: x[1], reverse=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8086b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{671.0: ['DeepSeek-R1'],\n",
       " 123.0: ['Mistral-Large-Instruct-2411'],\n",
       " 109.0: ['Llama-4-Scout-17B-16E-Instruct'],\n",
       " 72.0: ['Qwen2.5-72B-Instruct', 'Athene-V2-Chat'],\n",
       " 70.0: ['DeepSeek-R1-Distill-Llama-70B',\n",
       "  'Llama-3.1-70B-Instruct',\n",
       "  'Llama-3.3-70B-Instruct',\n",
       "  'Llama-3.1-Nemotron-70B-Instruct-HF',\n",
       "  'meditron-70b',\n",
       "  'MeLLaMA-70B-chat',\n",
       "  'Llama3-OpenBioLLM-70B',\n",
       "  'Llama-3-70B-UltraMedical'],\n",
       " 42.0: ['Phi-3.5-MoE-instruct'],\n",
       " 34.0: ['Yi-1.5-34B-Chat-16K'],\n",
       " 32.0: ['DeepSeek-R1-Distill-Qwen-32B', 'QwQ-32B-Preview', 'QWQ-32B'],\n",
       " 27.0: ['gemma-2-27b-it', 'gemma-3-27b-it'],\n",
       " 24.0: ['Mistral-Small-24B-Instruct-2501',\n",
       "  'Mistral-Small-3.1-24B-Instruct-2503'],\n",
       " 22.0: ['Mistral-Small-Instruct-2409'],\n",
       " 14.0: ['Baichuan-M1-14B-Instruct', 'DeepSeek-R1-Distill-Qwen-14B', 'Phi-4'],\n",
       " 13.0: ['MeLLaMA-13B-chat'],\n",
       " 12.0: ['gemma-3-12b-it'],\n",
       " 9.0: ['gemma-2-9b-it', 'Yi-1.5-9B-Chat-16K'],\n",
       " 8.0: ['DeepSeek-R1-Distill-Llama-8B',\n",
       "  'Llama-3.1-8B-Instruct',\n",
       "  'Llama3-OpenBioLLM-8B',\n",
       "  'MMed-Llama-3-8B',\n",
       "  'Llama-3.1-8B-UltraMedical',\n",
       "  'Ministral-8B-Instruct-2410'],\n",
       " 7.0: ['DeepSeek-R1-Distill-Qwen-7B',\n",
       "  'meditron-7b',\n",
       "  'BioMistral-7B',\n",
       "  'Qwen2.5-7B-Instruct'],\n",
       " 4.0: ['gemma-3-4b-it', 'Phi-3.5-mini-instruct'],\n",
       " 3.0: ['Llama-3.2-3B-Instruct', 'Qwen2.5-3B-Instruct'],\n",
       " 1.5: ['DeepSeek-R1-Distill-Qwen-1.5B', 'Qwen2.5-1.5B-Instruct'],\n",
       " 1.0: ['gemma-3-1b-it', 'Llama-3.2-1B-Instruct']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = {}\n",
    "\n",
    "for model, size in sorted_models:\n",
    "    if size not in grouped:\n",
    "        grouped[size] = []\n",
    "\n",
    "    grouped[size].append(model)\n",
    "\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "145bc00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{671.0: {'Direct Score': 44.25,\n",
       "  'CoT Score': 42.1,\n",
       "  'Difference': 2.15,\n",
       "  'Relative Difference': 4.86},\n",
       " 123.0: {'Direct Score': 42.28,\n",
       "  'CoT Score': 38.9,\n",
       "  'Difference': 3.37,\n",
       "  'Relative Difference': 7.97},\n",
       " 109.0: {'Direct Score': 35.12,\n",
       "  'CoT Score': 29.38,\n",
       "  'Difference': 5.74,\n",
       "  'Relative Difference': 16.34},\n",
       " 72.0: {'Direct Score': 41.66,\n",
       "  'CoT Score': 39.1,\n",
       "  'Difference': 2.56,\n",
       "  'Relative Difference': 6.15},\n",
       " 70.0: {'Direct Score': 33.23,\n",
       "  'CoT Score': 29.45,\n",
       "  'Difference': 3.78,\n",
       "  'Relative Difference': 11.38},\n",
       " 42.0: {'Direct Score': 29.54,\n",
       "  'CoT Score': 25.27,\n",
       "  'Difference': 4.27,\n",
       "  'Relative Difference': 14.45},\n",
       " 34.0: {'Direct Score': 32.12,\n",
       "  'CoT Score': 29.57,\n",
       "  'Difference': 2.55,\n",
       "  'Relative Difference': 7.94},\n",
       " 32.0: {'Direct Score': 36.95,\n",
       "  'CoT Score': 33.02,\n",
       "  'Difference': 3.94,\n",
       "  'Relative Difference': 10.66},\n",
       " 27.0: {'Direct Score': 39.06,\n",
       "  'CoT Score': 35.88,\n",
       "  'Difference': 3.17,\n",
       "  'Relative Difference': 8.12},\n",
       " 24.0: {'Direct Score': 38.63,\n",
       "  'CoT Score': 33.91,\n",
       "  'Difference': 4.71,\n",
       "  'Relative Difference': 12.19},\n",
       " 22.0: {'Direct Score': 35.19,\n",
       "  'CoT Score': 31.17,\n",
       "  'Difference': 4.02,\n",
       "  'Relative Difference': 11.42},\n",
       " 14.0: {'Direct Score': 35.5,\n",
       "  'CoT Score': 33.91,\n",
       "  'Difference': 1.58,\n",
       "  'Relative Difference': 4.45},\n",
       " 13.0: {'Direct Score': 20.76,\n",
       "  'CoT Score': 20.26,\n",
       "  'Difference': 0.5,\n",
       "  'Relative Difference': 2.41},\n",
       " 12.0: {'Direct Score': 37.32,\n",
       "  'CoT Score': 35.37,\n",
       "  'Difference': 1.95,\n",
       "  'Relative Difference': 5.23},\n",
       " 9.0: {'Direct Score': 31.94,\n",
       "  'CoT Score': 27.67,\n",
       "  'Difference': 4.27,\n",
       "  'Relative Difference': 13.37},\n",
       " 8.0: {'Direct Score': 23.76,\n",
       "  'CoT Score': 21.74,\n",
       "  'Difference': 2.02,\n",
       "  'Relative Difference': 8.5},\n",
       " 7.0: {'Direct Score': 21.63,\n",
       "  'CoT Score': 18.62,\n",
       "  'Difference': 3.02,\n",
       "  'Relative Difference': 13.96},\n",
       " 4.0: {'Direct Score': 26.99,\n",
       "  'CoT Score': 26.05,\n",
       "  'Difference': 0.93,\n",
       "  'Relative Difference': 3.45},\n",
       " 3.0: {'Direct Score': 24.74,\n",
       "  'CoT Score': 23.52,\n",
       "  'Difference': 1.22,\n",
       "  'Relative Difference': 4.93},\n",
       " 1.5: {'Direct Score': 18.21,\n",
       "  'CoT Score': 16.45,\n",
       "  'Difference': 1.76,\n",
       "  'Relative Difference': 9.66},\n",
       " 1.0: {'Direct Score': 14.22,\n",
       "  'CoT Score': 12.69,\n",
       "  'Difference': 1.53,\n",
       "  'Relative Difference': 10.76}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_size_performances(google_sheet, grouped_models):\n",
    "    '''\n",
    "    valid_models: list containing the __ 20% of models\n",
    "    '''\n",
    "    data = google_sheet.parse(\"All-Sheet\")\n",
    "    output = {}\n",
    "\n",
    "    for model_size, models_ls in grouped_models.items():\n",
    "        if model_size not in output:\n",
    "            output[model_size] = {}\n",
    "\n",
    "        # Compute the performances for all the models in that size range\n",
    "        valid_models = models_ls[:]\n",
    "\n",
    "        cot_scores = []\n",
    "        direct_scores = []\n",
    "\n",
    "        # want to determien the relationshpi between the average performance of a gorup of models AND how CoT/direct impacts\n",
    "        # Average the performance of all tasks with \n",
    "        for row_idx, model_name in enumerate(data[\"Model Name\"]):\n",
    "            # Must be in the valid models (correct size bucket)\n",
    "            if model_name in valid_models:\n",
    "                cot_scores.append(data[\"CoT Score\"][row_idx])\n",
    "                direct_scores.append(data[\"Direct Score\"][row_idx])\n",
    "\n",
    "        # Average the CoT and Direct Scores\n",
    "        avg_cot_score = np.mean(cot_scores)\n",
    "        avg_direct_score = np.mean(direct_scores)\n",
    "\n",
    "        avg_diff = round(avg_direct_score - avg_cot_score, 2)\n",
    "        avg_rel_diff = round(avg_diff / avg_direct_score * 100, 2)\n",
    "\n",
    "        # differences = []\n",
    "        # for i in range(len(direct_scores)):\n",
    "        #     differences.append(direct_scores[i] - cot_scores[i])\n",
    "\n",
    "        # relative_differences_list = []\n",
    "\n",
    "        # for i in range(len(differences)):\n",
    "        #     if direct_scores[i] == 0:\n",
    "        #         relative_differences_list.append(0)\n",
    "        #         continue\n",
    "        #     relative_differences_list.append( round(differences[i] / direct_scores[i] * 100, 2)   )\n",
    "\n",
    "        # avg_cot_perf = round(sum(cot_scores) / len(cot_scores), 2)\n",
    "        # avg_direct_perf = round(sum(direct_scores) / len(direct_scores), 2)\n",
    "\n",
    "        # Relative difference\n",
    "        # relative_diff = round(((avg_direct_perf - avg_cot_perf) / avg_direct_perf) * 100, 2)\n",
    "\n",
    "        output[model_size][\"Direct Score\"] = round(avg_direct_score, 2)\n",
    "        output[model_size][\"CoT Score\"] = round(avg_cot_score, 2)\n",
    "        output[model_size][\"Difference\"] = avg_diff\n",
    "        output[model_size][\"Relative Difference\"] = avg_rel_diff\n",
    "\n",
    "\n",
    "    return output\n",
    "\n",
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet-With-Invalid.xlsx\")\n",
    "\n",
    "get_size_performances(google_sheet, grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c02f6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size_out = {671.0: {'Direct Score': 44.25,\n",
    "  'CoT Score': 42.1,\n",
    "  'Difference': 2.15,\n",
    "  'Relative Difference': 4.86},\n",
    " 123.0: {'Direct Score': 42.28,\n",
    "  'CoT Score': 38.9,\n",
    "  'Difference': 3.37,\n",
    "  'Relative Difference': 7.97},\n",
    " 109.0: {'Direct Score': 35.12,\n",
    "  'CoT Score': 29.38,\n",
    "  'Difference': 5.74,\n",
    "  'Relative Difference': 16.34},\n",
    " 72.0: {'Direct Score': 41.66,\n",
    "  'CoT Score': 39.1,\n",
    "  'Difference': 2.56,\n",
    "  'Relative Difference': 6.15},\n",
    " 70.0: {'Direct Score': 33.23,\n",
    "  'CoT Score': 29.45,\n",
    "  'Difference': 3.78,\n",
    "  'Relative Difference': 11.38},\n",
    " 42.0: {'Direct Score': 29.54,\n",
    "  'CoT Score': 25.27,\n",
    "  'Difference': 4.27,\n",
    "  'Relative Difference': 14.45},\n",
    " 34.0: {'Direct Score': 32.12,\n",
    "  'CoT Score': 29.57,\n",
    "  'Difference': 2.55,\n",
    "  'Relative Difference': 7.94},\n",
    " 32.0: {'Direct Score': 36.95,\n",
    "  'CoT Score': 33.02,\n",
    "  'Difference': 3.94,\n",
    "  'Relative Difference': 10.66},\n",
    " 27.0: {'Direct Score': 39.06,\n",
    "  'CoT Score': 35.88,\n",
    "  'Difference': 3.17,\n",
    "  'Relative Difference': 8.12},\n",
    " 24.0: {'Direct Score': 38.63,\n",
    "  'CoT Score': 33.91,\n",
    "  'Difference': 4.71,\n",
    "  'Relative Difference': 12.19},\n",
    " 22.0: {'Direct Score': 35.19,\n",
    "  'CoT Score': 31.17,\n",
    "  'Difference': 4.02,\n",
    "  'Relative Difference': 11.42},\n",
    " 14.0: {'Direct Score': 35.5,\n",
    "  'CoT Score': 33.91,\n",
    "  'Difference': 1.58,\n",
    "  'Relative Difference': 4.45},\n",
    " 13.0: {'Direct Score': 20.76,\n",
    "  'CoT Score': 20.26,\n",
    "  'Difference': 0.5,\n",
    "  'Relative Difference': 2.41},\n",
    " 12.0: {'Direct Score': 37.32,\n",
    "  'CoT Score': 35.37,\n",
    "  'Difference': 1.95,\n",
    "  'Relative Difference': 5.23},\n",
    " 9.0: {'Direct Score': 31.94,\n",
    "  'CoT Score': 27.67,\n",
    "  'Difference': 4.27,\n",
    "  'Relative Difference': 13.37},\n",
    " 8.0: {'Direct Score': 23.76,\n",
    "  'CoT Score': 21.74,\n",
    "  'Difference': 2.02,\n",
    "  'Relative Difference': 8.5},\n",
    " 7.0: {'Direct Score': 21.63,\n",
    "  'CoT Score': 18.62,\n",
    "  'Difference': 3.02,\n",
    "  'Relative Difference': 13.96},\n",
    " 4.0: {'Direct Score': 26.99,\n",
    "  'CoT Score': 26.05,\n",
    "  'Difference': 0.93,\n",
    "  'Relative Difference': 3.45},\n",
    " 3.0: {'Direct Score': 24.74,\n",
    "  'CoT Score': 23.52,\n",
    "  'Difference': 1.22,\n",
    "  'Relative Difference': 4.93},\n",
    " 1.5: {'Direct Score': 18.21,\n",
    "  'CoT Score': 16.45,\n",
    "  'Difference': 1.76,\n",
    "  'Relative Difference': 9.66},\n",
    " 1.0: {'Direct Score': 14.22,\n",
    "  'CoT Score': 12.69,\n",
    "  'Difference': 1.53,\n",
    "  'Relative Difference': 10.76}}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(model_size_out).T\n",
    "\n",
    "df.to_csv(\"Revised Model Size vs. Performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171db0ef",
   "metadata": {},
   "source": [
    "___ \n",
    "\n",
    "## 13. Model Family Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c4d7b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_families = {\n",
    "    \"Baichuan\": [\"Baichuan-M1-14B-Instruct\"],\n",
    "\n",
    "    \"DeepSeek\": [\n",
    "        \"DeepSeek-R1\",\n",
    "        \"DeepSeek-R1-Distill-Llama-8B\",\n",
    "        \"DeepSeek-R1-Distill-Llama-70B\",\n",
    "        \"DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "        \"DeepSeek-R1-Distill-Qwen-7B\",\n",
    "        \"DeepSeek-R1-Distill-Qwen-14B\",\n",
    "        \"DeepSeek-R1-Distill-Qwen-32B\",\n",
    "    ],\n",
    "\n",
    "    \"Gemma\": [\n",
    "        \"gemma-2-9b-it\",\n",
    "        \"gemma-2-27b-it\",\n",
    "        \"gemma-3-1b-it\",\n",
    "        'gemma-3-4b-it',\n",
    "        'gemma-3-12b-it',\n",
    "        'gemma-3-27b-it',\n",
    "    ],\n",
    "\n",
    "    \"Llama\": [\n",
    "        'Llama-3.1-8B-Instruct',\n",
    "        'Llama-3.1-70B-Instruct',\n",
    "        'Llama-3.2-1B-Instruct',\n",
    "        'Llama-3.2-3B-Instruct',\n",
    "        'Llama-3.3-70B-Instruct',\n",
    "        'Llama-4-Scout-17B-16E-Instruct',\n",
    "        \"Llama-3.1-Nemotron-70B-Instruct-HF\",\n",
    "    ],\n",
    "\n",
    "    \"Meditron\": [\n",
    "        'meditron-7b',\n",
    "        'meditron-70b',\n",
    "    ],\n",
    "\n",
    "    \"MeLLaMA\": [\n",
    "        \"MeLLaMA-13B-chat\",\n",
    "        \"MeLLaMA-70B-chat\"\n",
    "    ],\n",
    "\n",
    "    \"Llama3-OpenBioLLM\": [\n",
    "        \"Llama3-OpenBioLLM-8B\",\n",
    "        \"Llama3-OpenBioLLM-70B\"\n",
    "    ],\n",
    "\n",
    "    \"MMed-Llama\": [\n",
    "        \"MMed-Llama-3-8B\"\n",
    "    ],\n",
    "\n",
    "    \"Llama-UltraMedical\": [\n",
    "        \"Llama-3.1-8B-UltraMedical\",\n",
    "        \"Llama-3-70B-UltraMedical\"\n",
    "    ],\n",
    "\n",
    "    \"Mistral\": [\n",
    "        \"Ministral-8B-Instruct-2410\",\n",
    "        \"Mistral-Small-Instruct-2409\",\n",
    "        \"Mistral-Small-24B-Instruct-2501\",\n",
    "        \"Mistral-Small-3.1-24B-Instruct-2503\",\n",
    "        \"Mistral-Large-Instruct-2411\"\n",
    "    ],\n",
    "\n",
    "    \"BioMistral\": [\n",
    "        \"BioMistral-7B\"\n",
    "    ],\n",
    "\n",
    "    \"Phi\": [\n",
    "        \"Phi-3.5-mini-instruct\",\n",
    "        \"Phi-3.5-MoE-instruct\",\n",
    "        \"Phi-4\"\n",
    "    ],\n",
    "\n",
    "    \"Qwen2.5\": [\n",
    "        \"Qwen2.5-1.5B-Instruct\",\n",
    "        \"Qwen2.5-3B-Instruct\",\n",
    "        \"Qwen2.5-7B-Instruct\",\n",
    "        \"Qwen2.5-72B-Instruct\"\n",
    "    ],\n",
    "\n",
    "    \"QwQ\": [\n",
    "        \"QwQ-32B-Preview\",\n",
    "        \"QWQ-32B\"\n",
    "    ],\n",
    "\n",
    "    \"Athene\": [\n",
    "        \"Athene-V2-Chat\"\n",
    "    ],\n",
    "\n",
    "    \"Yi\": [\n",
    "        \"Yi-1.5-9B-Chat-16K\",\n",
    "        \"Yi-1.5-34B-Chat-16K\"\n",
    "    ],\n",
    "\n",
    "    \"GPT\": [\n",
    "        \"gpt-35-turbo-0125\",\n",
    "        \"gpt-4o-0806\"\n",
    "    ],\n",
    "\n",
    "    \"Gemini\": [\n",
    "        \"gemini-2.0-flash-001\",\n",
    "        \"gemini-1.5-pro-002\"\n",
    "    ]\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9364fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_family_performances(google_sheet, families):\n",
    "    '''\n",
    "    valid_models: list containing the __ 20% of models\n",
    "    '''\n",
    "    data = google_sheet.parse(\"All-Sheet\")\n",
    "    output = {}\n",
    "\n",
    "    for model_family, models_ls in families.items():\n",
    "        if model_family not in output:\n",
    "            output[model_family] = {}\n",
    "\n",
    "        # Compute the performances for all the models in that size range\n",
    "        valid_models = models_ls[:]\n",
    "\n",
    "        cot_scores = []\n",
    "        direct_scores = []\n",
    "\n",
    "        # want to determien the relationshpi between the average performance of a gorup of models AND how CoT/direct impacts\n",
    "        # Average the performance of all tasks with \n",
    "        for row_idx, model_name in enumerate(data[\"Model Name\"]):\n",
    "            # Must be in the valid models\n",
    "            if model_name in valid_models:\n",
    "                cot_scores.append(data[\"CoT Score\"][row_idx])\n",
    "                direct_scores.append(data[\"Direct Score\"][row_idx])\n",
    "\n",
    "        avg_direct_score = np.mean(direct_scores)\n",
    "\n",
    "        avg_cot_score = np.mean(cot_scores)\n",
    "\n",
    "        avg_diff = avg_direct_score - avg_cot_score\n",
    "\n",
    "        avg_rel_diff = round ( avg_diff / avg_direct_score * 100, 2)\n",
    "\n",
    "        output[model_family][\"Direct Score\"] = round(avg_direct_score, 2)\n",
    "        output[model_family][\"CoT Score\"] = round(avg_cot_score, 2)\n",
    "        output[model_family][\"Difference\"] = round(avg_diff, 2)\n",
    "        output[model_family][\"Relative Difference\"] = round(avg_rel_diff, 2)\n",
    "        \n",
    "\n",
    "    return output\n",
    "\n",
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet-With-Invalid.xlsx\")\n",
    "\n",
    "family = get_family_performances(google_sheet, model_families)\n",
    "\n",
    "# Positive indicates Direct did better. Negative indicates CoT did better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c870e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(family).T\n",
    "df.to_csv(\"Revised Model Family vs. Performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e07f80",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 14. Clinical Context of Task Analysis\n",
    "\n",
    "Answers the question: How does CoT vs. Direct performance vary across different clinical contexts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d02faf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Get all clinical context: [tasks] mappings\n",
    "\n",
    "context_to_task_mapping = {}\n",
    "\n",
    "sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/Clinical Benchmark and LLM.xlsx\")\n",
    "\n",
    "data = sheet.parse(\"Task-all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9618453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_idx, clinical_contexts in enumerate(data[\"Clinical context\"]):\n",
    "\n",
    "    task = data[\"Task name\"][row_idx]\n",
    "\n",
    "    # Check if multiple contexts:\n",
    "    if \",\" in clinical_contexts:\n",
    "        clinical_contexts = clinical_contexts.split(\",\")\n",
    "\n",
    "        for context in clinical_contexts:\n",
    "            context = context.strip()\n",
    "            if context not in context_to_task_mapping:\n",
    "                context_to_task_mapping[context] = []\n",
    "\n",
    "            context_to_task_mapping[context].append(task)\n",
    "\n",
    "    else:\n",
    "        clinical_contexts = clinical_contexts.strip()\n",
    "        if clinical_contexts not in context_to_task_mapping:\n",
    "            context_to_task_mapping[clinical_contexts] = []\n",
    "\n",
    "        context_to_task_mapping[clinical_contexts].append(task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ad5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_performances(google_sheet, contexts):\n",
    "    '''\n",
    "    valid_models: list containing the __ 20% of models\n",
    "    '''\n",
    "    data = google_sheet.parse(\"All-Sheet\")\n",
    "    output = {}\n",
    "\n",
    "    for context, tasks_ls in contexts.items():\n",
    "        if context not in output:\n",
    "            output[context] = {}\n",
    "\n",
    "        valid_tasks = tasks_ls[:]\n",
    "\n",
    "        cot_scores = []\n",
    "        direct_scores = []\n",
    "\n",
    "        for row_idx, task_name in enumerate(data[\"Task Name\"]):\n",
    "            # Must be in the valid models\n",
    "            if task_name in valid_tasks:\n",
    "                cot_scores.append(data[\"CoT Score\"][row_idx])\n",
    "                direct_scores.append(data[\"Direct Score\"][row_idx])\n",
    "\n",
    "        # Compute Averages\n",
    "        avg_cot_score = np.mean(cot_scores)\n",
    "        avg_direct_score = np.mean(direct_scores)\n",
    "\n",
    "        avg_diff = avg_direct_score - avg_cot_score\n",
    "\n",
    "        avg_rel_diff = round( avg_diff / avg_direct_score * 100 , 2)\n",
    "\n",
    "        output[context][\"Direct Score\"] = round(avg_direct_score, 2)\n",
    "        output[context][\"CoT Score\"] = round(avg_cot_score, 2)\n",
    "        output[context][\"Difference\"] = round(avg_diff, 2)\n",
    "        output[context][\"Relative Difference\"] = avg_rel_diff\n",
    "\n",
    "    return output\n",
    "\n",
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet-With-Invalid.xlsx\")\n",
    "\n",
    "contexts = get_context_performances(google_sheet, context_to_task_mapping)\n",
    "\n",
    "# Positive indicates Direct did better. Negative indicates CoT did better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79a2e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(contexts).T\n",
    "df.to_csv(\"Revised Clinical Context vs. Performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ff7fd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pharmacology': ['ADE-Identification',\n",
       "  'ADE-Extraction',\n",
       "  'ADE-Drug dosage',\n",
       "  'DialMed',\n",
       "  'Medication extraction',\n",
       "  'n2c2 2018-ADE&medication',\n",
       "  'RuDReC-NER',\n",
       "  'IFMIR-Incident type',\n",
       "  'IFMIR-NER',\n",
       "  'IFMIR - NER&factuality',\n",
       "  'ADE-Identification',\n",
       "  'ADE-Extraction',\n",
       "  'ADE-Drug dosage',\n",
       "  'DialMed',\n",
       "  'Medication extraction',\n",
       "  'n2c2 2018-ADE&medication',\n",
       "  'RuDReC-NER',\n",
       "  'IFMIR-Incident type',\n",
       "  'IFMIR-NER',\n",
       "  'IFMIR - NER&factuality',\n",
       "  'ADE-Identification',\n",
       "  'ADE-Extraction',\n",
       "  'ADE-Drug dosage',\n",
       "  'DialMed',\n",
       "  'Medication extraction',\n",
       "  'n2c2 2018-ADE&medication',\n",
       "  'RuDReC-NER',\n",
       "  'IFMIR-Incident type',\n",
       "  'IFMIR-NER',\n",
       "  'IFMIR - NER&factuality'],\n",
       " 'General': ['BARR2',\n",
       "  'Brateca-Hospitalization',\n",
       "  'Brateca-Mortality',\n",
       "  'CHIP-CDEE',\n",
       "  'CodiEsp-ICD-10-CM',\n",
       "  'CodiEsp-ICD-10-PCS',\n",
       "  'ClinicalNotes-UPMC',\n",
       "  'cMedQA',\n",
       "  'EHRQA-Primary department',\n",
       "  'EHRQA-QA',\n",
       "  'EHRQA-Sub department',\n",
       "  'JP-STS',\n",
       "  'meddocan',\n",
       "  'MEDIQA 2019-RQE',\n",
       "  'MedSTS',\n",
       "  'MTS',\n",
       "  'NUBES',\n",
       "  'MEDIQA 2023-chat-A',\n",
       "  'MEDIQA 2023-sum-A',\n",
       "  'MEDIQA 2023-sum-B',\n",
       "  'RuMedDaNet',\n",
       "  'CBLUE-CDN',\n",
       "  'CHIP-CTC',\n",
       "  'CHIP-MDCFNPC',\n",
       "  'CAS-label',\n",
       "  'CAS-evidence',\n",
       "  'CLISTER',\n",
       "  'GraSSCo PHI',\n",
       "  'iCorpus',\n",
       "  'icliniq-10k',\n",
       "  'HealthCareMagic-100k',\n",
       "  'BARR2',\n",
       "  'Brateca-Hospitalization',\n",
       "  'Brateca-Mortality',\n",
       "  'CHIP-CDEE',\n",
       "  'CodiEsp-ICD-10-CM',\n",
       "  'CodiEsp-ICD-10-PCS',\n",
       "  'ClinicalNotes-UPMC',\n",
       "  'cMedQA',\n",
       "  'EHRQA-Primary department',\n",
       "  'EHRQA-QA',\n",
       "  'EHRQA-Sub department',\n",
       "  'JP-STS',\n",
       "  'meddocan',\n",
       "  'MEDIQA 2019-RQE',\n",
       "  'MedSTS',\n",
       "  'MTS',\n",
       "  'NUBES',\n",
       "  'MEDIQA 2023-chat-A',\n",
       "  'MEDIQA 2023-sum-A',\n",
       "  'MEDIQA 2023-sum-B',\n",
       "  'RuMedDaNet',\n",
       "  'CBLUE-CDN',\n",
       "  'CHIP-CTC',\n",
       "  'CHIP-MDCFNPC',\n",
       "  'CAS-label',\n",
       "  'CAS-evidence',\n",
       "  'CLISTER',\n",
       "  'GraSSCo PHI',\n",
       "  'iCorpus',\n",
       "  'icliniq-10k',\n",
       "  'HealthCareMagic-100k',\n",
       "  'BARR2',\n",
       "  'Brateca-Hospitalization',\n",
       "  'Brateca-Mortality',\n",
       "  'CHIP-CDEE',\n",
       "  'CodiEsp-ICD-10-CM',\n",
       "  'CodiEsp-ICD-10-PCS',\n",
       "  'ClinicalNotes-UPMC',\n",
       "  'cMedQA',\n",
       "  'EHRQA-Primary department',\n",
       "  'EHRQA-QA',\n",
       "  'EHRQA-Sub department',\n",
       "  'JP-STS',\n",
       "  'meddocan',\n",
       "  'MEDIQA 2019-RQE',\n",
       "  'MedSTS',\n",
       "  'MTS',\n",
       "  'NUBES',\n",
       "  'MEDIQA 2023-chat-A',\n",
       "  'MEDIQA 2023-sum-A',\n",
       "  'MEDIQA 2023-sum-B',\n",
       "  'RuMedDaNet',\n",
       "  'CBLUE-CDN',\n",
       "  'CHIP-CTC',\n",
       "  'CHIP-MDCFNPC',\n",
       "  'CAS-label',\n",
       "  'CAS-evidence',\n",
       "  'CLISTER',\n",
       "  'GraSSCo PHI',\n",
       "  'iCorpus',\n",
       "  'icliniq-10k',\n",
       "  'HealthCareMagic-100k'],\n",
       " 'Neurology': ['BrainMRI-AIS',\n",
       "  'CLINpt-NER',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD',\n",
       "  'BrainMRI-AIS',\n",
       "  'CLINpt-NER',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD',\n",
       "  'BrainMRI-AIS',\n",
       "  'CLINpt-NER',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD'],\n",
       " 'Radiology': ['BrainMRI-AIS',\n",
       "  'CARES-Area',\n",
       "  'CARES ICD10 Block',\n",
       "  'CARES-ICD10 Chapter',\n",
       "  'CARES-ICD10 Subblock',\n",
       "  'C-EMRS',\n",
       "  'DiSMed-NER',\n",
       "  'BrainMRI-AIS',\n",
       "  'CARES-Area',\n",
       "  'CARES ICD10 Block',\n",
       "  'CARES-ICD10 Chapter',\n",
       "  'CARES-ICD10 Subblock',\n",
       "  'C-EMRS',\n",
       "  'DiSMed-NER',\n",
       "  'BrainMRI-AIS',\n",
       "  'CARES-Area',\n",
       "  'CARES ICD10 Block',\n",
       "  'CARES-ICD10 Chapter',\n",
       "  'CARES-ICD10 Subblock',\n",
       "  'C-EMRS',\n",
       "  'DiSMed-NER'],\n",
       " 'Oncology': ['Cantemist-Coding',\n",
       "  'Cantemis-NER',\n",
       "  'Cantemis-Norm',\n",
       "  'BRONCO150-NER&Status',\n",
       "  'Cantemist-Coding',\n",
       "  'Cantemis-NER',\n",
       "  'Cantemis-Norm',\n",
       "  'BRONCO150-NER&Status',\n",
       "  'Cantemist-Coding',\n",
       "  'Cantemis-NER',\n",
       "  'Cantemis-Norm',\n",
       "  'BRONCO150-NER&Status'],\n",
       " 'Endocrinology': ['C-EMRS',\n",
       "  'GOUT-CC-Consensus',\n",
       "  'n2c2 2014-De-identification',\n",
       "  'n2c2 2014-Diabetes',\n",
       "  'n2c2 2014-CAD',\n",
       "  'n2c2 2014-Hyperlipidemia',\n",
       "  'n2c2 2014-Hypertension',\n",
       "  'n2c2 2014-Medication',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD',\n",
       "  'C-EMRS',\n",
       "  'GOUT-CC-Consensus',\n",
       "  'n2c2 2014-De-identification',\n",
       "  'n2c2 2014-Diabetes',\n",
       "  'n2c2 2014-CAD',\n",
       "  'n2c2 2014-Hyperlipidemia',\n",
       "  'n2c2 2014-Hypertension',\n",
       "  'n2c2 2014-Medication',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD',\n",
       "  'C-EMRS',\n",
       "  'GOUT-CC-Consensus',\n",
       "  'n2c2 2014-De-identification',\n",
       "  'n2c2 2014-Diabetes',\n",
       "  'n2c2 2014-CAD',\n",
       "  'n2c2 2014-Hyperlipidemia',\n",
       "  'n2c2 2014-Hypertension',\n",
       "  'n2c2 2014-Medication',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD'],\n",
       " 'Pulmonology': ['C-EMRS',\n",
       "  'PPTS',\n",
       "  'DialMed',\n",
       "  'n2c2 2006-De-identification',\n",
       "  'RuCCoN',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD',\n",
       "  'C-EMRS',\n",
       "  'PPTS',\n",
       "  'DialMed',\n",
       "  'n2c2 2006-De-identification',\n",
       "  'RuCCoN',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD',\n",
       "  'C-EMRS',\n",
       "  'PPTS',\n",
       "  'DialMed',\n",
       "  'n2c2 2006-De-identification',\n",
       "  'RuCCoN',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD'],\n",
       " 'Cardiology': ['C-EMRS',\n",
       "  'MIE',\n",
       "  'NorSynthClinical-NER',\n",
       "  'NorSynthClinical-RE',\n",
       "  'n2c2 2014-Diabetes',\n",
       "  'n2c2 2014-CAD',\n",
       "  'n2c2 2014-Hyperlipidemia',\n",
       "  'n2c2 2014-Hypertension',\n",
       "  'n2c2 2014-Medication',\n",
       "  'NorSynthClinical-PHI',\n",
       "  'CARDIO-DE',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD',\n",
       "  'C-EMRS',\n",
       "  'MIE',\n",
       "  'NorSynthClinical-NER',\n",
       "  'NorSynthClinical-RE',\n",
       "  'n2c2 2014-Diabetes',\n",
       "  'n2c2 2014-CAD',\n",
       "  'n2c2 2014-Hyperlipidemia',\n",
       "  'n2c2 2014-Hypertension',\n",
       "  'n2c2 2014-Medication',\n",
       "  'NorSynthClinical-PHI',\n",
       "  'CARDIO-DE',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD',\n",
       "  'C-EMRS',\n",
       "  'MIE',\n",
       "  'NorSynthClinical-NER',\n",
       "  'NorSynthClinical-RE',\n",
       "  'n2c2 2014-Diabetes',\n",
       "  'n2c2 2014-CAD',\n",
       "  'n2c2 2014-Hyperlipidemia',\n",
       "  'n2c2 2014-Hypertension',\n",
       "  'n2c2 2014-Medication',\n",
       "  'NorSynthClinical-PHI',\n",
       "  'CARDIO-DE',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD'],\n",
       " 'Gastroenterology': ['C-EMRS',\n",
       "  'DialMed',\n",
       "  'MedDG',\n",
       "  'MIMIC-IV CDM',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD',\n",
       "  'C-EMRS',\n",
       "  'DialMed',\n",
       "  'MedDG',\n",
       "  'MIMIC-IV CDM',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD',\n",
       "  'C-EMRS',\n",
       "  'DialMed',\n",
       "  'MedDG',\n",
       "  'MIMIC-IV CDM',\n",
       "  'MIMIC-IV DiReCT.Dis',\n",
       "  'MIMIC-IV DiReCT.PDD'],\n",
       " 'Critical Care': ['CLIP',\n",
       "  'n2c2 2010-Concept',\n",
       "  'n2c2 2010-Assertion',\n",
       "  'n2c2 2010-Relation',\n",
       "  'MedNLI',\n",
       "  'RuMedNLI',\n",
       "  'MIMIC-III Outcome.LoS',\n",
       "  'MIMIC-III Outcome.Mortality',\n",
       "  'MIMIC-IV BHC',\n",
       "  'CLIP',\n",
       "  'n2c2 2010-Concept',\n",
       "  'n2c2 2010-Assertion',\n",
       "  'n2c2 2010-Relation',\n",
       "  'MedNLI',\n",
       "  'RuMedNLI',\n",
       "  'MIMIC-III Outcome.LoS',\n",
       "  'MIMIC-III Outcome.Mortality',\n",
       "  'MIMIC-IV BHC',\n",
       "  'CLIP',\n",
       "  'n2c2 2010-Concept',\n",
       "  'n2c2 2010-Assertion',\n",
       "  'n2c2 2010-Relation',\n",
       "  'MedNLI',\n",
       "  'RuMedNLI',\n",
       "  'MIMIC-III Outcome.LoS',\n",
       "  'MIMIC-III Outcome.Mortality',\n",
       "  'MIMIC-IV BHC'],\n",
       " 'Dermatology': ['DialMed', 'DialMed', 'DialMed'],\n",
       " 'Nephrology': ['Ex4CDS', 'Ex4CDS', 'Ex4CDS'],\n",
       " 'Pediatrics': ['IMCS-V2-NER',\n",
       "  'MTS-Temporal',\n",
       "  'IMCS-V2-SR',\n",
       "  'IMCS-V2-MRG',\n",
       "  'IMCS-V2-DAC',\n",
       "  'IMCS-V2-NER',\n",
       "  'MTS-Temporal',\n",
       "  'IMCS-V2-SR',\n",
       "  'IMCS-V2-MRG',\n",
       "  'IMCS-V2-DAC',\n",
       "  'IMCS-V2-NER',\n",
       "  'MTS-Temporal',\n",
       "  'IMCS-V2-SR',\n",
       "  'IMCS-V2-MRG',\n",
       "  'IMCS-V2-DAC'],\n",
       " 'Psychology': ['MTS-Temporal', 'MTS-Temporal', 'MTS-Temporal']}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_to_task_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f8f41c",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## 15. Combine All Tasks (Sections 5 and 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4f2d58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_sheet = pd.ExcelFile(\"New-Prompt-Sheet.xlsx\")\n",
    "\n",
    "out = {\n",
    "    \"Task\": [],\n",
    "    \"Task Type\": [],\n",
    "    \"Direct Score\": [],\n",
    "    \"CoT Score\": [],\n",
    "    \"Direct - Cot\": [],\n",
    "    \"Relative Difference\": []\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f3aaaa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_performances(google_sheet, all_sheets):\n",
    "    all_task_info = {}\n",
    "    \n",
    "    for sheet in all_sheets:\n",
    "        data = google_sheet.parse(sheet)\n",
    "        # print(data)\n",
    "\n",
    "        for row_idx, task_name in enumerate(data[\"Task Name\"]):\n",
    "            if task_name in all_english_tasks:\n",
    "                if task_name not in all_task_info:\n",
    "                    all_task_info[task_name] = {}\n",
    "                    all_task_info[task_name][\"Task Type\"] = data[\"Task Type\"][row_idx]\n",
    "                    all_task_info[task_name][\"Direct Score\"] = []\n",
    "                    all_task_info[task_name][\"CoT Score\"] = []\n",
    "                    all_task_info[task_name][\"Direct - Cot\"] = []\n",
    "                    all_task_info[task_name][\"Relative Difference\"] = []\n",
    "\n",
    "                all_task_info[task_name][\"Direct Score\"].append(data[\"Direct Score\"][row_idx])\n",
    "                all_task_info[task_name][\"CoT Score\"].append(data[\"CoT Score\"][row_idx])\n",
    "                all_task_info[task_name][\"Direct - Cot\"].append(data[\"Difference\"][row_idx])\n",
    "                all_task_info[task_name][\"Relative Difference\"].append(data[\"Relative Difference\"][row_idx])\n",
    "\n",
    "\n",
    "    return all_task_info\n",
    "\n",
    "\n",
    "out = get_task_performances(google_sheet, [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\", \"Prompt 4\",])\n",
    "\n",
    "with open(\"section15_output_english_only_1234.json\", \"w\") as f:\n",
    "    json.dump(out, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "16c754f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_table(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    out = {\n",
    "        \"Task\": [],\n",
    "        \"Task Type\": [],\n",
    "        \"Direct Score\": [],\n",
    "        \"CoT Score\": [],\n",
    "        \"Direct - Cot\": [],\n",
    "        \"Relative Difference\": []\n",
    "    }\n",
    "\n",
    "    # For each task, get the average of the scores and add to the final output\n",
    "    for task_name in data:\n",
    "        out[\"Task\"].append(task_name)\n",
    "\n",
    "        for task_attr in data[task_name]:\n",
    "            # if task_attr == \"Relative Difference\":  # skip this calculation\n",
    "            #     continue\n",
    "\n",
    "            if task_attr == \"Task Type\":\n",
    "                out[task_attr].append(data[task_name][task_attr])\n",
    "                continue\n",
    "\n",
    "            out[task_attr].append(round(np.mean(data[task_name][task_attr]), 2))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "avg_out = create_final_table(\"section15_output_english_only_1234.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f1e47d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into CSV Dataframe\n",
    "df = pd.DataFrame(avg_out)\n",
    "df.to_csv(\"section15_output_english_only_1234.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb52c6",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 16. All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b1e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task name to simplified task name mapping\n",
    "google_sheet = pd.ExcelFile(\"Clinical Benchmark and LLM.xlsx\")\n",
    "\n",
    "data = google_sheet.parse(\"Task-all\")\n",
    "\n",
    "task_name_mapping = {}\n",
    "\n",
    "for row_idx, task_name in enumerate(data[\"Task-Original\"]):\n",
    "    task_name_mapping[data[\"Task name\"][row_idx]] = data[\"Task name\"][row_idx]\n",
    "    task_name_mapping[task_name] = data[\"Task name\"][row_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b8e4ae09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DeepSeek-R1', 44.25), ('gpt-4o-0806', 44.2), ('gemini-1.5-pro-002', 43.85), ('gemini-2.0-flash-001', 43.03), ('Mistral-Large-Instruct-2411', 42.17), ('Athene-V2-Chat', 41.29), ('Qwen2.5-72B-Instruct', 41.2), ('gemma-3-27b-it', 39.9), ('Llama-3.3-70B-Instruct', 39.9), ('DeepSeek-R1-Distill-Llama-70B', 39.79), ('DeepSeek-R1-Distill-Qwen-32B', 39.75), ('Mistral-Small-3.1-24B-Instruct-2503', 39.73), ('QWQ-32B', 39.37), ('Llama-3.1-70B-Instruct', 39.09), ('gemma-2-27b-it', 38.22), ('Mistral-Small-24B-Instruct-2501', 37.53), ('gemma-3-12b-it', 37.32), ('Phi-4', 36.13), ('Baichuan-M1-14B-Instruct', 36.08), ('gpt-35-turbo-0125', 35.3), ('Mistral-Small-Instruct-2409', 35.19), ('Llama-4-Scout-17B-16E-Instruct', 35.12), ('gemma-2-9b-it', 35.07), ('DeepSeek-R1-Distill-Qwen-14B', 34.28), ('Llama-3-70B-UltraMedical', 33.4), ('Llama3-OpenBioLLM-70B', 33.01), ('Llama-3.1-Nemotron-70B-Instruct-HF', 32.75), ('MeLLaMA-70B-chat', 32.26), ('Yi-1.5-34B-Chat-16K', 32.12), ('QwQ-32B-Preview', 31.74), ('Qwen2.5-7B-Instruct', 31.32), ('Ministral-8B-Instruct-2410', 30.37), ('Phi-3.5-MoE-instruct', 29.54), ('Llama-3.1-8B-Instruct', 28.98), ('Yi-1.5-9B-Chat-16K', 28.81), ('gemma-3-4b-it', 28.56), ('DeepSeek-R1-Distill-Llama-8B', 28.48), ('Qwen2.5-3B-Instruct', 26.59), ('Phi-3.5-mini-instruct', 25.41), ('DeepSeek-R1-Distill-Qwen-7B', 25.27), ('Llama-3.2-3B-Instruct', 22.9), ('Qwen2.5-1.5B-Instruct', 22.16), ('MeLLaMA-13B-chat', 20.76), ('BioMistral-7B', 20.43), ('MMed-Llama-3-8B', 20.37), ('Llama-3.1-8B-UltraMedical', 20.16), ('gemma-3-1b-it', 15.73), ('meditron-70b', 15.68), ('DeepSeek-R1-Distill-Qwen-1.5B', 14.26), ('Llama3-OpenBioLLM-8B', 14.2), ('Llama-3.2-1B-Instruct', 12.72), ('meditron-7b', 9.52)]\n",
      "[('DeepSeek-R1', 42.1), ('gpt-4o-0806', 40.66), ('gemini-1.5-pro-002', 40.53), ('gemini-2.0-flash-001', 41.98), ('Mistral-Large-Instruct-2411', 39.06), ('Athene-V2-Chat', 39.27), ('Qwen2.5-72B-Instruct', 38.87), ('gemma-3-27b-it', 37.55), ('Llama-3.3-70B-Instruct', 37.3), ('DeepSeek-R1-Distill-Llama-70B', 38.95), ('DeepSeek-R1-Distill-Qwen-32B', 38.72), ('Mistral-Small-3.1-24B-Instruct-2503', 36.23), ('QWQ-32B', 37.03), ('Llama-3.1-70B-Instruct', 35.1), ('gemma-2-27b-it', 34.22), ('Mistral-Small-24B-Instruct-2501', 31.59), ('gemma-3-12b-it', 35.37), ('Phi-4', 32.59), ('Baichuan-M1-14B-Instruct', 34.36), ('gpt-35-turbo-0125', 31.63), ('Mistral-Small-Instruct-2409', 31.17), ('Llama-4-Scout-17B-16E-Instruct', 29.38), ('gemma-2-9b-it', 29.94), ('DeepSeek-R1-Distill-Qwen-14B', 34.79), ('Llama-3-70B-UltraMedical', 29.44), ('Llama3-OpenBioLLM-70B', 28.78), ('Llama-3.1-Nemotron-70B-Instruct-HF', 24.09), ('MeLLaMA-70B-chat', 29.25), ('Yi-1.5-34B-Chat-16K', 29.57), ('QwQ-32B-Preview', 23.31), ('Qwen2.5-7B-Instruct', 30.25), ('Ministral-8B-Instruct-2410', 25.91), ('Phi-3.5-MoE-instruct', 25.27), ('Llama-3.1-8B-Instruct', 29.4), ('Yi-1.5-9B-Chat-16K', 25.4), ('gemma-3-4b-it', 28.19), ('DeepSeek-R1-Distill-Llama-8B', 27.34), ('Qwen2.5-3B-Instruct', 25.44), ('Phi-3.5-mini-instruct', 23.91), ('DeepSeek-R1-Distill-Qwen-7B', 23.87), ('Llama-3.2-3B-Instruct', 21.6), ('Qwen2.5-1.5B-Instruct', 19.47), ('MeLLaMA-13B-chat', 20.26), ('BioMistral-7B', 10.84), ('MMed-Llama-3-8B', 16.17), ('Llama-3.1-8B-UltraMedical', 18.34), ('gemma-3-1b-it', 13.53), ('meditron-70b', 13.17), ('DeepSeek-R1-Distill-Qwen-1.5B', 13.42), ('Llama3-OpenBioLLM-8B', 13.29), ('Llama-3.2-1B-Instruct', 11.86), ('meditron-7b', 9.52)]\n"
     ]
    }
   ],
   "source": [
    "def get_model_performance(google_sheet, isCoT):\n",
    "    data = google_sheet.parse(\"Revised-All-Sheet\")\n",
    "\n",
    "    results = {}  \n",
    "\n",
    "    for row_idx, model_name in enumerate(data[\"Model Name\"]):\n",
    "        # model_name = task_name_mapping[model_name]\n",
    "        if model_name not in results:\n",
    "            results[model_name] = []\n",
    "\n",
    "        task = data[\"Task Name\"][row_idx]\n",
    "\n",
    "        if isCoT:\n",
    "            cot_score = data[\"CoT Score\"][row_idx]\n",
    "            results[model_name].append(cot_score)\n",
    "\n",
    "        else:\n",
    "            direct_score = data[\"Direct Score\"][row_idx]\n",
    "            results[model_name].append(direct_score)\n",
    "\n",
    "    for key in results:\n",
    "        results[key] = round(np.mean(results[key]), 2)\n",
    "\n",
    "    return sorted(results.items(), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "google_sheet = load_sheet(\"/Users/kevinxie/Desktop/LLM CoT/LLM-CoT/CoT-Difference-Sheet-With-Invalid.xlsx\")\n",
    "\n",
    "direct_performances = get_model_performance(google_sheet, False)\n",
    "cot_performances = get_model_performance(google_sheet, True)\n",
    "\n",
    "\n",
    "# SORT COT TO HAVE THE SAME ORDER\n",
    "# Create a name-to-index mapping from the reference list\n",
    "order = {name: idx for idx, (name, _) in enumerate(direct_performances)}\n",
    "\n",
    "# Step 2: Reorder cot_performance using those indices\n",
    "sorted_cot = sorted(cot_performances, key=lambda x: order.get(x[0], float('inf')))\n",
    "\n",
    "\n",
    "# Sorted by direct scores\n",
    "print(direct_performances)\n",
    "print(sorted_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f6344a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {\n",
    "    \"Model\": [],\n",
    "    \"Model Size\": [],\n",
    "    \"Model Type\": [],\n",
    "    \"Model Domain\": [],\n",
    "    \"Direct Score\": [],\n",
    "    \"CoT Score\": [],\n",
    "    \"Difference\": [],\n",
    "    \"Relative Difference\": []\n",
    "}\n",
    "\n",
    "\n",
    "# Add Direct and Cot Scores\n",
    "for model, score in direct_performances:\n",
    "    table[\"Model\"].append(model)\n",
    "    table[\"Model Size\"].append(model_size_mapping[model])\n",
    "    table[\"Model Type\"].append(accessibility_mapping[model])\n",
    "    table[\"Model Domain\"].append(model_domain_mapping[model])\n",
    "    table[\"Direct Score\"].append(score)\n",
    "\n",
    "for model, score in sorted_cot:\n",
    "    table[\"CoT Score\"].append(score)\n",
    "\n",
    "# Fix Accessibility stuff\n",
    "for i, access in enumerate(table[\"Model Type\"]):\n",
    "    if access == 'commercial':\n",
    "        table[\"Model Type\"][i] = \"Commercial\"\n",
    "\n",
    "    elif access == \"open source\":\n",
    "        table[\"Model Type\"][i] = \"Open-Source\"\n",
    "        \n",
    "\n",
    "# Calculate Differences!\n",
    "\n",
    "for idx, direct_score in enumerate(table[\"Direct Score\"]):\n",
    "    diff = round(table[\"CoT Score\"][idx] - direct_score, 2)\n",
    "\n",
    "    table['Difference'].append(diff)\n",
    "\n",
    "    relative_diff = round( (diff / direct_score) * 100, 2 )\n",
    "\n",
    "    table[\"Relative Difference\"].append(relative_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22de91da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table)\n",
    "df.to_csv(\"Overall-LLM-Performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e7c5a",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 17. Revised Multi-Prompt Analysis\n",
    "\n",
    "Instead of analyzing individual task performances (i.e. 1-1.ADE-ADE Identification), we analyze the overall changes in performance across all NLP Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0d51147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_nlp_task(sheet_data):\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    for row_idx, task_type in enumerate(sheet_data[\"Task Type\"]):\n",
    "        if task_type not in out:\n",
    "            out[task_type] = {}\n",
    "            out[task_type][\"Direct Score\"] = []\n",
    "            out[task_type][\"CoT Score\"] = []\n",
    "\n",
    "\n",
    "        dir_score = sheet_data[\"Direct Score\"][row_idx]\n",
    "        cot_score = sheet_data[\"CoT Score\"][row_idx]\n",
    "\n",
    "        out[task_type][\"Direct Score\"].append(dir_score)\n",
    "        out[task_type][\"CoT Score\"].append(cot_score)\n",
    "        \n",
    "\n",
    "    for task_type, d in out.items():\n",
    "        # out[k] = round( np.mean(out[k]), 2 )\n",
    "        d[\"Difference\"] = []\n",
    "\n",
    "        for idx, dir_value in enumerate(d['Direct Score']):\n",
    "            d[\"Difference\"].append( dir_value - d['CoT Score'][idx ])\n",
    "\n",
    "\n",
    "        out[task_type]['Relative Difference'] = []\n",
    "\n",
    "        for idx, diff_value in enumerate(out[task_type][\"Difference\"]):\n",
    "            dir_score = out[task_type][\"Direct Score\"][idx]\n",
    "\n",
    "            if dir_score == 0:\n",
    "                continue\n",
    "            out[task_type][\"Relative Difference\"].append( round( diff_value / dir_score * 100, 2) )\n",
    "\n",
    "\n",
    "    final_out = {}\n",
    "    # Compute Averages\n",
    "    for task_type, d in out.items():\n",
    "        final_out[task_type] = {}\n",
    "\n",
    "        # for key, values_ls in d.items():\n",
    "        final_out[task_type][\"Direct Score\"] = round(np.mean( d[\"Direct Score\"] ), 2)\n",
    "\n",
    "        final_out[task_type][\"CoT Score\"] = round( np.mean( d[\"CoT Score\"] ), 2)\n",
    "\n",
    "        final_out[task_type][\"Difference\"] = round( -np.mean( d[\"Difference\"] ), 2)\n",
    "\n",
    "    for task_type, d in final_out.items():\n",
    "        diff = final_out[task_type][\"Direct Score\"] - final_out[task_type][\"CoT Score\"]\n",
    "        dir_score = final_out[task_type][\"Direct Score\"]\n",
    "\n",
    "        final_out[task_type][\"Relative Difference\"] = round( -diff / dir_score * 100, 2)\n",
    "\n",
    "\n",
    "    return final_out\n",
    "\n",
    "\n",
    "all_prompt_sheet = pd.ExcelFile(\"New-Prompt-Sheet.xlsx\")\n",
    "\n",
    "current_prompt = \"prompt_4\"\n",
    "all_prompt_data = all_prompt_sheet.parse(current_prompt)\n",
    "\n",
    "out = get_by_nlp_task(all_prompt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "907910d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(out).T\n",
    "\n",
    "df.to_csv(f\"Revised Multiprompt NLP-{current_prompt}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18e7ce28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text Classification': {'Direct Score': 67.0,\n",
       "  'CoT Score': 66.05,\n",
       "  'Difference': -0.95,\n",
       "  'Relative Difference': -1.42},\n",
       " 'Event Extraction': {'Direct Score': 26.28,\n",
       "  'CoT Score': 21.08,\n",
       "  'Difference': -5.2,\n",
       "  'Relative Difference': -19.79},\n",
       " 'Named Entity Recognition': {'Direct Score': 39.31,\n",
       "  'CoT Score': 37.02,\n",
       "  'Difference': -2.29,\n",
       "  'Relative Difference': -5.83},\n",
       " 'Question Answering': {'Direct Score': 17.12,\n",
       "  'CoT Score': 15.8,\n",
       "  'Difference': -1.32,\n",
       "  'Relative Difference': -7.71},\n",
       " 'Summarization': {'Direct Score': 32.21,\n",
       "  'CoT Score': 28.35,\n",
       "  'Difference': -3.87,\n",
       "  'Relative Difference': -11.98},\n",
       " 'Normalization and Coding': {'Direct Score': 5.45,\n",
       "  'CoT Score': 6.2,\n",
       "  'Difference': 0.74,\n",
       "  'Relative Difference': 13.76},\n",
       " 'Semantic Similarity': {'Direct Score': 45.27,\n",
       "  'CoT Score': 44.2,\n",
       "  'Difference': -1.07,\n",
       "  'Relative Difference': -2.36},\n",
       " 'Natural Language Inference': {'Direct Score': 83.56,\n",
       "  'CoT Score': 83.46,\n",
       "  'Difference': -0.1,\n",
       "  'Relative Difference': -0.12}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804a47a",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## 18. Revised CoT Token Length Analysis\n",
    "\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. For each model-task pair output file, determine the token lengths of each output. Then, split those into the 10 buckets, each representing 10 percentiles of output length.\n",
    "    - this is impossible given my current data...\n",
    "\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a19796",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
